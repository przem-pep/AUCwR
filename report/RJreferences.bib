% put bibtex entries here!
@Book{plotly,
    author = {Carson Sievert},
    title = {Interactive Web-Based Data Visualization with R, plotly, and shiny},
    publisher = {Chapman and Hall/CRC},
    year = {2020},
    isbn = {9781138331457},
    url = {https://plotly-r.com},
  }

@Manual{crosstalk,
  title = {{crosstalk}: Inter-Widget Interactivity for HTML Widgets},
  author = {Joe Cheng and Carson Sievert},
  year = {2021},
  note = {R package version 1.1.1},
  url = {https://CRAN.R-project.org/package=crosstalk},
}

@article{RJ-2021-050,
  author = {Earo Wang and Dianne Cook},
  title = {Conversations in time: interactive visualisation to explore
          structured temporal data},
  year = {2021},
  journal = {The R Journal},
  doi = {10.32614/RJ-2021-050},
  url = {https://journal.r-project.org/archive/2021/RJ-2021-050/index.html}
}

@article{Kochanski_2021,
 title={A Simulation Model for Risk and Pricing Competition in the Retail Lending Market},
 volume={71},
 ISSN={2464-7683},
 DOI={10.32065/CJEF.2021.02.01},
 abstractNote={We propose a simulation model of the retail lending market with two types of agents: borrowers searching for low interest rates and lenders competing through risk-based pricing. We show that individual banks observe adverse selection, even if every lender applies the same pricing strategy and a credit scoring model of comparable discrimination power. Additionally, the model justifies the reverse-S shape of the response rate curve. According to the model, the benefits of even small increases in the discrimination power of credit scoring are substantial. This effect is more pronounced if the number of offers checked by the applicants before making a decision increases. The simulations illustrate the trade-off between profitability, market share, and credit loss rates. The profit-maximising strategy is to set interest rates slightly lower than the competition; the excessive price reduction turns out to be counterproductive. At the same time, there exists a niche for higher yield players.},
 number={2},
 journal={Czech Journal of Economics and Finance},
 author={Kocha\'{n}ski, Błażej},
 year={2021},
 month=oct,
 pages={96–118},
 language={en}
}

@article{Vargha_Delaney_2000,
 title={A Critique and Improvement of the "CL" Common Language Effect Size Statistics of McGraw and Wong},
 volume={25},
 ISSN={1076-9986},
 DOI={10.2307/1165329},
 abstractNote={McGraw and Wong (1992) described an appealing index of effect size, called "CL", which measures the difference between two populations in terms of the probability that a score sampled at random from the first population will be greater than a score sampled at random from the second. McGraw and Wong introduced this "common language effect size statistic" for normal distributions and then proposed an approximate estimation for any continuous distribution. In addition, they generalized "CL" to the n-group case, the correlated samples case, and the discrete values case. In the current paper a different generalization of "CL" called the A measure of stochastic superiority, is proposed, which may be directly applied for any discrete or continuous variable that is at least ordinally scaled. Exact methods for point and interval estimation as well as the significance tests of the A = .5 hypothesis are provided. New generalizations of "CL" are provided for the multi-group and correlated samples cases.},
 number={2},
 journal={Journal of Educational and Behavioral Statistics},
 publisher={[American Educational Research Association, Sage Publications, Inc., American Statistical Association]},
 author={Vargha, Andr\'{a}s and Delaney, Harold D.},
 year={2000},
 pages={101–132}
}

@article{Cliff_1993,
 address={US},
 title={Dominance statistics: Ordinal analyses to answer ordinal questions},
 volume={114},
 ISSN={1939-1455},
 DOI={10.1037/0033-2909.114.3.494},
 abstractNote={Much behavioral research involves comparing the central tendencies of different groups, or of the same Ss under different conditions, and the usual analysis is some form of mean comparison. This article suggests that an ordinal statistic, d, is often more appropriate. d compares the number of times a score from one group or condition is higher than one from the other, compared with the reverse. Compared to mean comparisons, d is more robust and equally or more powerful; it is invariant under transformation; and it often conforms more closely to the experimeter's research hypothesis. It is suggested that inferences from d be based on sample estimates of its variance rather than on the more traditional assumption of identical distributions. The statistic is extended to simple repeated measures designs, and ways of extending its use to more complex designs are suggested. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
 number={3},
 journal={Psychological Bulletin},
 publisher={American Psychological Association},
 author={Cliff, Norman},
 year={1993},
 pages={494–509}
}

@article{Idczak_2019,
 title={Remarks on Statistical Measures for Assessing Quality of Scoring Models},
 volume={4},
 rights={Copyright (c) 2019},
 ISSN={2353-7663},
 DOI={10.18778/0208-6018.343.02},
 abstractNote={Granting a credit product has always been at the heart of banking. Simultaneously, banks are obligated to assess the borrower's credit risk. Apart from creditworthiness, to grant a credit product, banks are using credit scoring more and more often. Scoring models, which are an essential part of credit scoring, are being developed in order to select those clients who will repay their debt. For lenders, high effectiveness of selection based on the scoring model is the primary attribute, so it is crucial to gauge its statistical quality. Several textbooks regarding assessing statistical quality of scoring models are available, there is however no full consistency between names and definitions of particular measures. In this article, the most common statistical measures for assessing quality of scoring models, such as the pseudo Gini index, Kolmogorov‑Smirnov statistic, and concentration curve are reviewed and their statistical characteristics are discussed. Furthermore, the author proposes the application of the well‑known distribution similarity index as a measure of discriminatory power of scoring models. The author also attempts to standardise names and formulas for particular measures in order to finally contrast them in a comparative analysis of credit scoring models.},
 number={343343},
 journal={Acta Universitatis Lodziensis. Folia Oeconomica},
 author={Idczak, Adam Piotr},
 year={2019},
 month=sep,
 pages={21–38},
 language={en}
}

@article{Bamber_1975,
 title={The area above the ordinal dominance graph and the area below the receiver operating characteristic graph},
 volume={12},
 rights={https://www.elsevier.com/tdm/userlicense/1.0/},
 ISSN={00222496},
 DOI={10.1016/0022-2496(75)90001-2},
 number={4},
 journal={Journal of Mathematical Psychology},
 author={Bamber, Donald},
 year={1975},
 month=nov,
 pages={387–415},
 language={en}
}

@article{Harrell_Califf,
 title={Evaluating the Yield of Medical Tests},
 author={Harrell, Frank E and Califf, Robert M},
 language={en}
}

@article{Hanley_McNeil_1982,
 title={The meaning and use of the area under a receiver operating characteristic (ROC) curve.},
 volume={143},
 ISSN={0033-8419},
 DOI={10.1148/radiology.143.1.7063747},
 abstractNote={A representation and interpretation of the area under a receiver operating characteristic (ROC) curve obtained by the "rating" method, or by mathematical predictions based on patient characteristics, is presented. It is shown that in such a setting the area represents the probability that a randomly chosen diseased subject is (correctly) rated or ranked with greater suspicion than a randomly chosen non-diseased subject. Moreover, this probability of a correct ranking is the same quantity that is estimated by the already well-studied nonparametric Wilcoxon statistic. These two relationships are exploited to (a) provide rapid closed-form expressions for the approximate magnitude of the sampling variability, i.e., standard error that one uses to accompany the area under a smoothed ROC curve, (b) guide in determining the size of the sample required to provide a sufficiently reliable estimate of this area, and (c) determine how large sample sizes should be to ensure that one can statistically detect differences in the accuracy of diagnostic techniques.},
 number={1},
 journal={Radiology},
 publisher={Radiological Society of North America},
 author={Hanley, J A and McNeil, B J},
 year={1982},
 month=apr,
 pages={29–36}
}

@article{Engelmann_Hayden_Tasche_2003,
 title={Measuring the Discriminative Power of Rating Systems},
 ISSN={1556-5068},
 url={https://www.ssrn.com/abstract=2793951},
 DOI={10.2139/ssrn.2793951},
 abstractNote={Assessing the discriminative power of rating systems is an important question to banks and to regulators. In this article we analyze the Cumulative Accuracy Proﬁle (CAP) and the Receiver Operating Characteristic (ROC) which are both commonly used in practice. We give a test-theoretic interpretation for the concavity of the CAP and the ROC curve and demonstrate how this observation can be used for more efﬁciently exploiting the informational contents of accounting ratios. Furthermore, we show that two popular summary statistics of these concepts, namely the Accuracy Ratio and the area under the ROC curve, contain the same information and we analyse the statistical properties of these measures. We show in detail how to identify accounting ratios with high discriminative power, how to calculate conﬁdence intervals for the area below the ROC curve, and how to test if two rating models validated on the same data set are different. All concepts are illustrated by applications to real data.},
 journal={SSRN Electronic Journal},
 author={Engelmann, Bernd and Hayden, Evelyn and Tasche, Dirk},
 year={2003},
 language={en}
}

@article{Newson_2002,
 title={Parameters behind "Nonparametric" Statistics: Kendall's tau, Somers' D and Median Differences},
 volume={2},
 rights={https://journals.sagepub.com/page/policies/text-and-data-mining-license},
 ISSN={1536-867X, 1536-8734},
 DOI={10.1177/1536867X0200200103},
 abstractNote={So-called "non-parametric" statistical methods are often in fact based on population parameters, which can be estimated (with conﬁdence limits) using the corresponding sample statistics. This article reviews the uses of three such parameters, namely Kendall's τa, Somers' D and the Hodges-Lehmann median diﬀerence. Conﬁdence intervals for these are demonstrated using the somersd package. It is argued that conﬁdence limits for these parameters, and their differences, are more informative than the traditional practice of reporting only P -values. These three parameters are also important in deﬁning other tests and parameters, such as the Wilcoxon test, the area under the receiver operating characteristic (ROC) curve, Harrell's C, and the Theil median slope.},
 number={1},
 journal={The Stata Journal: Promoting communications on statistics and Stata},
 author={Newson, Roger},
 year={2002},
 month=mar,
 pages={45–64},
 language={en}
}

@article{Somers_1962,
 title={A New Asymmetric Measure of Association for Ordinal Variables},
 volume={27},
 ISSN={00031224},
 DOI={10.2307/2090408},
 number={6},
 journal={American Sociological Review},
 author={Somers, Robert H.},
 year={1962},
 month=dec,
 pages={799},
 language={en}
}

@article{Austin_Steyerberg_2012,
 title={Interpreting the concordance statistic of a logistic regression model: relation to the variance and odds ratio of a continuous explanatory variable},
 volume={12},
 rights={http://creativecommons.org/licenses/by/2.0},
 ISSN={1471-2288},
 DOI={10.1186/1471-2288-12-82},
 abstractNote={Background: When outcomes are binary, the c-statistic (equivalent to the area under the Receiver Operating Characteristic curve) is a standard measure of the predictive accuracy of a logistic regression model.
 Methods: An analytical expression was derived under the assumption that a continuous explanatory variable follows a normal distribution in those with and without the condition. We then conducted an extensive set of Monte Carlo simulations to examine whether the expressions derived under the assumption of binormality allowed for accurate prediction of the empirical c-statistic when the explanatory variable followed a normal distribution in the combined sample of those with and without the condition. We also examine the accuracy of the predicted cstatistic when the explanatory variable followed a gamma, log-normal or uniform distribution in combined sample of those with and without the condition.
 Results: Under the assumption of binormality with equality of variances, the c-statistic follows a standard normal cumulative distribution function with dependence on the product of the standard deviation of the normal components (reflecting more heterogeneity) and the log-odds ratio (reflecting larger effects). Under the assumption of binormality with unequal variances, the c-statistic follows a standard normal cumulative distribution function with dependence on the standardized difference of the explanatory variable in those with and without the condition. In our Monte Carlo simulations, we found that these expressions allowed for reasonably accurate prediction of the empirical c-statistic when the distribution of the explanatory variable was normal, gamma, log-normal, and uniform in the entire sample of those with and without the condition.
 Conclusions: The discriminative ability of a continuous explanatory variable cannot be judged by its odds ratio alone, but always needs to be considered in relation to the heterogeneity of the population.},
 number={1},
 journal={BMC Medical Research Methodology},
 author={Austin, Peter C and Steyerberg, Ewout W},
 year={2012},
 month=dec,
 pages={82},
 language={en}
}

@article{Bais_Van_Der_Neut_2022,
 title={Adapting the Robust Effect Size Cliff's Delta to Compare Behaviour Profiles},
 rights={Creative Commons Attribution Non Commercial 4.0 International},
 DOI={10.18148/SRM/2022.V16I2.7908},
 abstractNote={Cliff's Delta is a non-parametric effect size that is based on data observations. In this paper, we introduce an adaptation of Cliff's Delta in order to compare behaviour profiles. Behaviour profiles are density distributions in which survey answer behaviour is summarized for specific groups of respondents or items. Such profiles are useful, as they take into account the varying number of survey items that is filled out per respondent due to filter questions. By the adapted Cliff's Delta, two subgroups of respondents (for instance higher and lower educated respondents) can be compared on the occurrence of specific answer behaviour (for instance giving ‘don't know'-answers). By means of simulations, we show that the adapted profile-based Cliff's Delta converges towards the original observation-based Cliff's Delta as the number of items that is filled out by respondents increases. We conclude that the profile-based Cliff's Delta is a solid and conservative statistic that is both useful and advantageous to compare behaviour profiles. We close with various data examples to illustrate its broad usefulness and by discussing potential difficulty in using the profile-based Cliff's Delta.},
 journal={Survey Research Methods},
 publisher={Survey Research Methods},
 author={Bais, Frank and Van Der Neut, Joost},
 year={2022},
 month=dec,
 pages={329-352 Pages},
 language={en}
}

@article{Brunner_Munzel_2000,
  title   = {The Nonparametric Behrens-Fisher Problem: Asymptotic Theory and a Small-Sample Approximation},
  author  = {Brunner, Edgar and Munzel, Ullrich},
  journal = {Biometrical Journal},
  volume  = {42},
  number  = {1},
  pages   = {17--25},
  year    = {2000},
  issn    = {1521-4036},
  doi     = {10.1002/(SICI)1521-4036(200001)42:1\%3C17::AID-BIMJ17\%3E3.0.CO;2-U},
  language= {en}
}
 
 


@article{Brunner_Munzel_Puri_2002,
 title={The multivariate nonparametric Behrens–Fisher problem},
 volume={108},
 rights={https://www.elsevier.com/tdm/userlicense/1.0/},
 ISSN={03783758},
 DOI={10.1016/S0378-3758(02)00269-0},
 abstractNote={In this paper, we consider the multivariate case of the so-called nonparametric Behrens–Fisher problem where two samples with independent multivariate observations are given and the equality of the marginal distribution functions under the hypothesis in the two groups is not assumed. Moreover, we do not require the continuity of the marginal distribution functions so that data with ties and, particularly, multivariate-ordered categorical data are covered by this model. A multivariate relative treatment e3ect is de4ned which can be estimated by using the mid-ranks of the observations within each component and we derive the asymptotic distribution of this estimator. Moreover, the unknown asymptotic covariance matrix of the centered vector of the estimated relative treatment e3ects is estimated and its L2-consistency is proved. To test the hypothesis of no treatment e3ect, we consider the rank version of the Wald-type statistic (as used in Puri and Sen, Nonparametric Methods in Multivariate Analysis, Wiley, New York, 1971) and the rank version of the ANOVA-type statistic which was suggested by Brunner et al. [J. Amer. Statist. Assoc. 92 (1997) 1494–1502] for univariate nonparametric models. Simulations show that the ANOVA-type statistic appears to maintain the pre-assigned level of the test quite accurately (even for rather small sample sizes) while the Wald-type statistic leads to more or less liberal decisions. Regarding the power, none of the two statistics is uniformly superior to the other.},
 number={1–2},
 journal={Journal of Statistical Planning and Inference},
 author={Brunner, Edgar and Munzel, Ullrich and Puri, Madan L.},
 year={2002},
 month=nov,
 pages={37–53},
 language={en}
}

@article{Hand_Till_2001,
 title={A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems},
 abstractNote={The area under the ROC curve, or the equivalent Gini index, is a widely used measure of performance of supervised classiﬁcation rules. It has the attractive property that it side-steps the need to specify the costs of the different kinds of misclassiﬁcation. However, the simple form is only applicable to the case of two classes. We extend the deﬁnition to the case of more than two classes by averaging pairwise comparisons. This measure reduces to the standard form in the two class case. We compare its properties with the standard measure of proportion correct and an alternative deﬁnition of proportion correct based on pairwise comparison of classes for a simple artiﬁcial case and illustrate its application on eight data sets. On the data sets we examined, the measures produced similar, but not identical results, reﬂecting the different aspects of performance that they were measuring. Like the area under the ROC curve, the measure we propose is useful in those many situations where it is impossible to give costs for the different kinds of misclassiﬁcation.},
 author={Hand, David J and Till, Robert J},
 year={2001},
 language={en}
}

@article{Harrell_Califf_Pryor_Lee_Rosati_1982,
 title={Evaluating the Yield of Medical Tests},
 volume={247},
 ISSN={0098-7484},
 DOI={10.1001/jama.1982.03320430047030},
 abstractNote={A method is presented for evaluating the amount of information a medical test provides about individual patients. Emphasis is placed on the role of a test in the evaluation of patients with a chronic disease. In this context, the yield of a test is best interpreted by analyzing the prognostic information it furnishes. Information from the history, physical examination, and routine procedures should be used in assessing the yield of a new test. As an example, the method is applied to the use of the treadmill exercise test in evaluating the prognosis of patients with suspected coronary artery disease. The treadmill test is shown to provide surprisingly little prognostic information beyond that obtained from basic clinical measurements.(JAMA 1982;247:2543-2546)},
 number={18},
 journal={JAMA},
 author={Harrell, Frank E., Jr and Califf, Robert M. and Pryor, David B. and Lee, Kerry L. and Rosati, Robert A.},
 year={1982},
 month=may,
 pages={2543–2546}
}

@article{McGraw_Wong_1992,
 address={US},
 title={A common language effect size statistic},
 volume={111},
 ISSN={1939-1455},
 DOI={10.1037/0033-2909.111.2.361},
 abstractNote={Some of the shortcomings in interpretability and generalizability of the effect size statistics currently available to researchers can be overcome by a statistic that expresses how often a score sampled from one distribution will be greater than a score sampled from another distribution. The statistic, the common language effect size indicator, is easily calculated from sample means and variances (or from proportions in the case of nominal-level data). It can be used for expressing the effect observed in both independent and related sample designs and in both 2-group and n-group designs. Empirical tests show it to be robust to violations of the normality assumption, particularly when the variances in the 2 parent distributions are equal. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
 number={2},
 journal={Psychological Bulletin},
 publisher={American Psychological Association},
 author={McGraw, Kenneth O. and Wong, S. P.},
 year={1992},
 pages={361–365}
}

@article{LeDell_Petersen_Van_Der_Laan_2015,
 title={Computationally efficient confidence intervals for cross-validated area under the ROC curve estimates},
 volume={9},
 ISSN={1935-7524},
 url={https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-9/issue-1/Computationally-efficient-confidence-intervals-for-cross-validated-area-under-the/10.1214/15-EJS1035.full},
 DOI={10.1214/15-EJS1035},
 abstractNote={In binary classification problems, the area under the ROC curve (AUC) is commonly used to evaluate the performance of a prediction model. Often, it is combined with cross-validation in order to assess how the results will generalize to an independent data set. In order to evaluate the quality of an estimate for cross-validated AUC, we obtain an estimate of its variance. For massive data sets, the process of generating a single performance estimate can be computationally expensive. Additionally, when using a complex prediction method, the process of cross-validating a predictive model on even a relatively small data set can still require a large amount of computation time. Thus, in many practical settings, the bootstrap is a computationally intractable approach to variance estimation. As an alternative to the bootstrap, we demonstrate a computationally efficient influence curve based approach to obtaining a variance estimate for cross-validated AUC.},
 number={1},
 journal={Electronic Journal of Statistics},
 author={LeDell, Erin and Petersen, Maya and Van Der Laan, Mark},
 year={2015},
 month=jan,
 language={en}
}

@article{Noma_Matsushima_Ishii_2021,
 title={Confidence interval for the AUC of SROC curve and some related methods using bootstrap for meta-analysis of diagnostic accuracy studies},
 volume={7},
 ISSN={2373-7484},
 DOI={10.1080/23737484.2021.1894408},
 abstractNote={The area under the curve (AUC) of summary receiver operating characteristic (SROC) curve is a primary statistical outcome for meta-analysis of diagnostic test accuracy studies (DTA). However, its confidence interval has not been reported in most of DTA meta-analyses, because no certain methods and statistical packages have been provided. In this article, we provide a bootstrap algorithm for computing the confidence interval of the AUC. Also, using the bootstrap framework, we can conduct a bootstrap test for assessing significance of the difference of AUCs for two diagnostic tests. In addition, we provide an influence diagnostic method based on the AUC by leave-one-study-out analyses. We present illustrative examples using two DTA met-analyses for diagnostic tests of cervical cancer and asthma. We also developed an easy-to-handle R package dmetatools for these computations. The various quantitative evidence provided by these methods certainly supports the interpretations and precise evaluations of statistical evidence of DTA meta-analyses.},
 number={3},
 journal={Communications in Statistics: Case Studies, Data Analysis and Applications},
 author={Noma, Hisashi and Matsushima, Yuki and Ishii, Ryota},
 year={2021},
 month=jul,
 pages={344–358},
 language={en}
}

@article{Bandos_Rockette_Gur_2006,
 title={A Permutation Test for Comparing ROC Curves in Multireader Studies},
 volume={13},
 rights={https://www.elsevier.com/tdm/userlicense/1.0/},
 ISSN={10766332},
 DOI={10.1016/j.acra.2005.12.012},
 number={4},
 journal={Academic Radiology},
 author={Bandos, Andriy I. and Rockette, Howard E. and Gur, David},
 year={2006},
 month=apr,
 pages={414–420},
 language={en}
}

@article{Pauly_Asendorf_Konietschke_2016,
 title={Permutation‐based inference for the AUC: A unified approach for continuous and discontinuous data},
 volume={58},
 rights={http://onlinelibrary.wiley.com/termsAndConditions#vor},
 ISSN={0323-3847, 1521-4036},
 DOI={10.1002/bimj.201500105},
 abstractNote={We investigate rank‐based studentized permutation methods for the nonparametric Behrens–Fisher problem, that is, inference methods for the area under the ROC curve. We hereby prove that the studentized permutation distribution of the Brunner‐Munzel rank statistic is asymptotically standard normal, even under the alternative. Thus, incidentally providing the hitherto missing theoretical foundation for the Neubert and Brunner studentized permutation test. In particular, we do not only show its consistency, but also that confidence intervals for the underlying treatment effects can be computed by inverting this permutation test. In addition, we derive permutation‐based range‐preserving confidence intervals. Extensive simulation studies show that the permutation‐based confidence intervals appear to maintain the preassigned coverage probability quite accurately (even for rather small sample sizes). For a convenient application of the proposed methods, a freely available software package for the statistical software R has been developed. A real data example illustrates the application.},
 number={6},
 journal={Biometrical Journal},
 author={Pauly, Markus and Asendorf, Thomas and Konietschke, Frank},
 year={2016},
 month=nov,
 pages={1319–1337},
 language={en}
}

@article{Janitza_Strobl_Boulesteix_2013,
 title={An AUC-based permutation variable importance measure for random forests},
 volume={14},
 rights={http://creativecommons.org/licenses/by/2.0},
 ISSN={1471-2105},
 DOI={10.1186/1471-2105-14-119},
 abstractNote={Background: The random forest (RF) method is a commonly used tool for classification with high dimensional data as well as for ranking candidate predictors based on the so-called random forest variable importance measures (VIMs). However the classification performance of RF is known to be suboptimal in case of strongly unbalanced data, i.e. data where response class sizes differ considerably. Suggestions were made to obtain better classification performance based either on sampling procedures or on cost sensitivity analyses. However to our knowledge the performance of the VIMs has not yet been examined in the case of unbalanced response classes. In this paper we explore the performance of the permutation VIM for unbalanced data settings and introduce an alternative permutation VIM based on the area under the curve (AUC) that is expected to be more robust towards class imbalance.
 Results: We investigated the performance of the standard permutation VIM and of our novel AUC-based permutation VIM for different class imbalance levels using simulated data and real data. The results suggest that the new AUC-based permutation VIM outperforms the standard permutation VIM for unbalanced data settings while both permutation VIMs have equal performance for balanced data settings.
 Conclusions: The standard permutation VIM loses its ability to discriminate between associated predictors and predictors not associated with the response for increasing class imbalance. It is outperformed by our new AUC-based permutation VIM for unbalanced data settings, while the performance of both VIMs is very similar in the case of balanced classes. The new AUC-based VIM is implemented in the R package party for the unbiased RF variant based on conditional inference trees. The codes implementing our study are available from the companion website: http://www.ibe.med.uni-muenchen.de/organisation/mitarbeiter/070_drittmittel/janitza/index.html.},
 number={1},
 journal={BMC Bioinformatics},
 author={Janitza, Silke and Strobl,
 Carolin and Boulesteix, Anne-Laure},
 year={2013},
 month=dec,
 pages={119},
 language={en}
}

@Article{Biecek_2018,
    title = {DALEX: Explainers for Complex Predictive Models in R},
    author = {Przemyslaw Biecek},
    journal = {Journal of Machine Learning Research},
    year = {2018},
    volume = {19},
    pages = {1-5},
    number = {84},
    url = {https://jmlr.org/papers/v19/18-416.html},
  }
  
@article{Yang_Ying_2023,
 title={AUC Maximization in the Era of Big Data and AI: A Survey},
 volume={55},
 ISSN={0360-0300, 1557-7341},
 DOI={10.1145/3554729},
 abstractNote={Area under the ROC curve, a.k.a. AUC, is a measure of choice for assessing the performance of a classifier for imbalanced data. AUC maximization refers to a learning paradigm that learns a predictive model by directly maximizing its AUC score. It has been studied for more than two decades dating back to late 90s and a huge amount of work has been devoted to AUC maximization since then. Recently, stochastic AUC maximization for big data and deep AUC maximization (DAM) for deep learning have received increasing attention and yielded dramatic impact for solving real-world problems. However, to the best our knowledge there is no comprehensive survey of related works for AUC maximization. This paper aims to address the gap by reviewing the literature in the past two decades. We not only give a holistic view of the literature but also present detailed explanations and comparisons of different papers from formulations to algorithms and theoretical guarantees. We also identify and discuss remaining and emerging issues for DAM, and provide suggestions on topics for future work. CCS Concepts: - Computing methodologies -> Machine learning algorithms; - Theory of computation -> Continuous optimization; Stochastic control and optimization.},
 number={8},
 journal={ACM Computing Surveys},
 author={Yang, Tianbao and Ying, Yiming},
 year={2023},
 month=aug,
 pages={1–37},
 language={en}
}

@article{Kochanski_2022,
 title={Which Curve Fits Best: Fitting ROC Curve Models to Empirical Credit-Scoring Data},
 volume={10},
 rights={https://creativecommons.org/licenses/by/4.0/},
 ISSN={2227-9091},
 DOI={10.3390/risks10100184},
 abstractNote={In the practice of credit-risk management, the models for receiver operating characteristic (ROC) curves are helpful in describing the shape of an ROC curve, estimating the discriminatory power of a scorecard, and generating ROC curves without underlying data. The primary purpose of this study is to review the ROC curve models proposed in the literature, primarily in biostatistics, and to ﬁt them to actual credit-scoring ROC data in order to determine which models could be used in credit-risk-management practice. We list several theoretical models for an ROC curve and describe them in the credit-scoring context. The model list includes the binormal, bigamma, bibeta, bilogistic, power, and bifractal curves. The models are then tested against empirical credit-scoring ROC data from publicly available presentations and papers, as well as from European retail lending institutions. Except for the power curve, all the presented models ﬁt the data quite well. However, based on the results and other favourable properties, it is suggested that the binormal curve is the preferred choice for modelling credit-scoring ROC curves.},
 number={10},
 journal={Risks},
 author={Kocha\'{n}ski, B{\l}a\'{z}ej},
 year={2022},
 month=sep,
 pages={184},
 language={en}
}

@article{Junge_Dettori_2018,
 title={ROC Solid: Receiver Operator Characteristic (ROC) Curves as a Foundation for Better Diagnostic Tests},
 volume={8},
 ISSN={2192-5682},
 DOI={10.1177/2192568218778294},
 number={4},
 journal={Global Spine Journal},
 author={Junge, Mark R. J. and Dettori, Joseph R.},
 year={2018},
 month=jun,
 pages={424–429}
}

@misc{Gromada_2016,
 title={Receiver Operating Characteristic – Krzywa ROC – czyli ocena jakości klasyfikacji (cz\k{e}\'{s}\'{c} 7) – MathSpace.pl},
 url={https://mathspace.pl/matematyka/receiver-operating-characteristic-krzywa-roc-czyli-ocena-jakosci-klasyfikacji-czesc-7/},
 author={Gromada, Mariusz},
 year={2016},
 month=sep,
 language={pl-PL}
}

@article{Fawcett_2006,
 title={An introduction to ROC analysis},
 volume={27},
 rights={https://www.elsevier.com/tdm/userlicense/1.0/},
 ISSN={01678655}, DOI={10.1016/j.patrec.2005.10.010},
 abstractNote={Receiver operating characteristics (ROC) graphs are useful for organizing classiﬁers and visualizing their performance. ROC graphs are commonly used in medical decision making, and in recent years have been used increasingly in machine learning and data mining research. Although ROC graphs are apparently simple, there are some common misconceptions and pitfalls when using them in practice. The purpose of this article is to serve as an introduction to ROC graphs and as a guide for using them in research.},
 number={8},
 journal={Pattern Recognition Letters},
 author={Fawcett, Tom},
 year={2006},
 month=jun,
 pages={861–874},
 language={en}
}

@article{Karch_2021, 
 address={US}, 
 title={Psychologists should use Brunner-Munzel’s instead of Mann-Whitney’s U test as the default nonparametric procedure}, 
 volume={4}, ISSN={2515-2467}, DOI={10.1177/2515245921999602}, 
 abstractNote={To investigate whether a variable tends to be larger in one population than in another, the t test is the standard procedure. In some situations, the parametric t test is inappropriate, and a nonparametric procedure should be used instead. The default nonparametric procedure is Mann-Whitney’s U test. Despite being a nonparametric test, Mann-Whitney’s test is associated with a strong assumption, known as exchangeability. I demonstrate that if exchangeability is violated, Mann-Whitney’s test can lead to wrong statistical inferences even for large samples. In addition, I argue that in psychology, exchangeability is typically not met. As a remedy, I introduce Brunner-Munzel’s test and demonstrate that it provides good Type I error rate control even if exchangeability is not met and that it has similar power as Mann-Whitney’s test. Consequently, I recommend using Brunner-Munzel’s test by default. To facilitate this, I provide advice on how to perform and report on Brunner-Munzel’s test. (PsycInfo Database Record (c) 2021 APA, all rights reserved)}, number={2}, journal={Advances in Methods and Practices in Psychological Science}, 
 publisher={Sage Publications}, 
 author={Karch, Julian D.}, 
 year={2021} }
 
 
@article{Kharin_Zwiers_2003, 
  title={On the ROC Score of Probability Forecasts}, 
  volume={16}, 
  ISSN={0894-8755, 1520-0442}, 
  DOI={10.1175/1520-0442(2003)016\%3C4145:OTRSOP\%3E2.0.CO;2}, 
  number={24}, journal={Journal of Climate}, publisher={American Meteorological Society}, author={Kharin, Viatcheslav V. and Zwiers, Francis W.}, year={2003}, month=dec, pages={4145–4150}, language={EN} }

@article{Mason_Graham_1999, 
title={Conditional Probabilities, Relative Operating Characteristics, and Relative Operating Levels}, 
volume={14}, ISSN={1520-0434, 0882-8156}, 
DOI={10.1175/1520-0434(1999)014\%3C0713:CPROCA\%3E2.0.CO;2}, abstractNote={The relative operating characteristic (ROC) curve is a highly flexible method for representing the quality of dichotomous, categorical, continuous, and probabilistic forecasts. The method is based on ratios that measure the proportions of events and nonevents for which warnings were provided. These ratios provide estimates of the probabilities that an event will be forewarned and that an incorrect warning will be provided for a nonevent. Some guidelines for interpreting the ROC curve are provided. While the ROC curve is of direct interest to the user, the warning is provided in advance of the outcome and so there is additional value in knowing the probability of an event occurring contingent upon a warning being provided or not provided. An alternative method to the ROC curve is proposed that represents forecast quality when expressed in terms of probabilities of events occurring contingent upon the warnings provided. The ratios used provide estimates of the probability of an event occurring given the forecast that is issued. Some problems in constructing the curve in a manner that is directly analogous to that for the ROC curve are highlighted, and so an alternative approach is proposed. In the context of probabilistic forecasts, the ROC curve provides a means of identifying the forecast probability at which forecast value is optimized. In the context of continuous variables, the proposed relative operating levels curve indicates the exceedence threshold for defining an event at which forecast skill is optimized, and can enable the forecast user to estimate the probabilities of events other than that defined by the forecaster.}, number={5}, journal={Weather and Forecasting}, publisher={American Meteorological Society}, author={Mason, Simon J. and Graham, Nicholas E.}, year={1999}, month=oct, pages={713–725}, language={EN} }


@article{Wendt_1972, 
  title={Dealing with a common problem in Social science: A simplified rank-biserial coefficient of correlation based on the U statistic}, volume={2}, 
  ISSN={1099-0992}, 
  DOI={10.1002/ejsp.2420020412}, number={4}, journal={European Journal of Social Psychology}, 
  author={Wendt, Hans W.}, year={1972}, pages={463–465}, language={en} }
  
  
@article{Rubia_2022, 
 title={Note on Rank-Biserial Correlation when There Are Ties}, volume={12}, 
 rights={http://creativecommons.org/licenses/by/4.0/}, 
 DOI={10.4236/ojs.2022.125036}, abstractNote={The objective of this article is to demonstrate with examples that the two-sided tie correction does not work well. This correction was developed by Cureton so that Kendall’s tau-type and Spearman’s rho-type formulas for rank-biserial correlation yield the same result when ties are present. However, a correction based on the bracket ties achieves the desired goal, which is demonstrated algebraically and checked with three examples. On the one hand, the 10-element random sample given by Cureton, in which the two-sided tie correction performs well, is taken up. On the other hand, two other examples are given, one with a 7-element random sample and the other with a clinical random sample of 31 participants, in which the two-sided tie correction does not work, but the new correction does. It is concluded that the new corrected formulas coincide with Goodman-Kruskal’s gamma as compared to Glass’ formula that matches Somers’ dY|X or asymmetric measure of association of Y ranking with respect to X dichotomy. The use of this underreported coefficient is suggested, which is very easy to calculate from its equivalence with Kruskal-Wallis’ gamma and Somers’ dY|X.}, number={55}, journal={Open Journal of Statistics}, publisher={Scientific Research Publishing}, 
 author={Rubia, José Moral de la}, year={2022}, month=oct, pages={597–622}, language={en} }

@book{Sobehart_Keenan_Stein_2000, 
 title={Benchmarking Quantitative Default Risk Models: A Validation Methodology}, 
 url={https://www.researchgate.net/publication/243771528_Benchmarking_Quantitative_Default_Risk_Models_A_Validation_Methodology}, abstractNote={Benchmarking Quantitative Default Risk Models: A Validation Methodology}, institution={Moody’s Investor Service. Global Credit Research}, author={Sobehart, Jorge R. and Keenan, Sean C. and Stein, Roger M.}, year={2000}, language={en} }

@article{Rezac_Rezac_2011, 
title={How to Measure the Quality of Credit Scoring Models}, 
volume={61}, abstractNote={Credit scoring models are widely used to predict the probability of client default. To measure the quality of such scoring models it is possible to use quantitative indices such as the Gini index, Kolmogorov-Smirnov statistics (KS), Lift, the Mahalanobis distance, and information statistics. This paper reviews and illustrates the use of these indices in practice.}, 
number={5}, journal={Czech Journal of Economics and Finance (Finance a úvěr)}, publisher={Charles University Prague, Faculty of Social Sciences}, 
author={Řezáč, Martin and Řezáč, František}, year={2011}, pages={486–507}, language={en} }

@book{Agresti_2010, 
 title={Analysis of Ordinal Categorical Data}, ISBN={978-0-470-08289-8}, 
 abstractNote={Statistical science’s first coordinated manual of methods for analyzing ordered categorical data, now fully revised and updated, continues to present applications and case studies in fields as diverse as sociology, public health, ecology, marketing, and pharmacy. Analysis of Ordinal Categorical Data, Second Edition provides an introduction to basic descriptive and inferential methods for categorical data, giving thorough coverage of new developments and recent methods. Special emphasis is placed on interpretation and application of methods including an integrated comparison of the available strategies for analyzing ordinal data. Practitioners of statistics in government, industry (particularly pharmaceutical), and academia will want this new edition.}, 
 note={Google-Books-ID: VVIe4BPDR7kC}, 
 publisher={John Wiley \& Sons}, author={Agresti, Alan}, year={2010}, month=apr, language={en} }

@book{Anderson_2007, 
 address={Oxford}, 
 title={The Credit Scoring Toolkit: Theory and Practice for Retail Credit Risk Management and Decision Automation}, 
 ISBN={978-0-19-922640-5}, 
 abstractNote={The Credit Scoring Toolkit provides an all-encompassing view of the use of statistical models to assess retail credit risk and provide automated decisions.In eight modules, the book provides frameworks for both theory and practice. It first explores the economic justification and history of Credit Scoring, risk linkages and decision science, statistical and mathematical tools, the assessment of business enterprises, and regulatory issues ranging from data privacy to Basel II. It then provides a practical how-to-guide for scorecard development, including data collection, scorecard implementation, and use within the credit risk management cycle.Including numerous real-life examples and an extensive glossary and bibliography, the text assumes little prior knowledge making it an indispensable desktop reference for graduate students in statistics, business, economics and finance, MBA students, credit risk and financial practitioners.}, 
 note={Google-Books-ID: 7LlGfPvOJLoC}, 
 publisher={OUP Oxford}, author={Anderson, Raymond}, year={2007}, month=aug, language={en} }
 
@article{Simonoff_Hochberg_Reiser_1986, title={Alternative Estimation Procedures for Pr(X < Y) in Categorized Data}, volume={42}, 
 ISSN={0006-341X}, DOI={10.2307/2530703}, 
 abstractNote={Consider two independent random variables X and Y. The functional R = Pr(X < Y) [or λ = Pr(X < Y) - Pr(Y < X)] is of practical importance in many situations, including clinical trials, genetics, and reliability. In this paper several approaches to estimation of λ when X and Y are presented in discretized (Categorical) form are analyzed and compared. Asymptotic formulas for the variance of the estimators are derived; use of the bootstrap to estimate variances is also discussed. Computer simulations indicate that the choice of the best estimator depends on the value of λ, the underlying distribution, and the sparseness of the data. It is shown that the bootstrap provides a robust estimate of variance. Several examples are treated.}, number={4}, journal={Biometrics}, publisher={International Biometric Society}, author={Simonoff, Jeffrey S. and Hochberg, Yosef and Reiser, Benjamin}, year={1986}, pages={895–907} }

@article{Venkatraman_2000, title={A Permutation Test to Compare Receiver Operating Characteristic Curves}, 
 volume={56}, ISSN={1541-0420}, DOI={10.1111/j.0006-341X.2000.01134.x}, abstractNote={Summary. We developed a permutation test in our earlier paper (Venkatraman and Begg, 1996, Biometrika83, 835–848) to test the equality of receiver operating characteristic curves based on continuous paired data. Here we extend the underlying concepts to develop a permutation test for continuous unpaired data, and we study its properties through simulations.}, number={4}, journal={Biometrics}, author={Venkatraman, E. S.}, year={2000}, pages={1134–1138}, language={en} }

@article{Tsamardinos_Greasidou_Borboudakis_2018, 
 title={Bootstrapping the out-of-sample predictions for efficient and accurate cross-validation}, volume={107}, 
 ISSN={0885-6125, 1573-0565}, DOI={10.1007/s10994-018-5714-4}, 
 number={12}, journal={Machine Learning}, author={Tsamardinos, Ioannis and Greasidou, Elissavet and Borboudakis, Giorgos}, year={2018}, month=dec, pages={1895–1922}, language={en} }

@article{Gu_Ghosal_Roy_2008, title={Bayesian bootstrap estimation of ROC curve}, volume={27}, rights={Copyright © 2008 John Wiley & Sons, Ltd.}, ISSN={1097-0258}, DOI={10.1002/sim.3366}, abstractNote={Receiver operating characteristic (ROC) curve is widely applied in measuring discriminatory ability of diagnostic or prognostic tests. This makes the ROC analysis one of the most active research areas in medical statistics. Many parametric and semiparametric estimation methods have been proposed for estimating the ROC curve and its functionals. In this paper, we propose the Bayesian bootstrap (BB), a fully nonparametric estimation method, for the ROC curve and its functionals, such as the area under the curve (AUC). The BB method offers a bandwidth-free smoothing approach to the empirical estimate, and gives credible bounds. The accuracy of the estimate of the ROC curve in the simulation studies is examined by the integrated absolute error. In comparison with other existing curve estimation methods, the BB method performs well in terms of accuracy, robustness and simplicity. We also propose a procedure based on the BB approach to test the binormality assumption. Copyright © 2008 John Wiley & Sons, Ltd.}, number={26}, journal={Statistics in Medicine}, author={Gu, Jiezhun and Ghosal, Subhashis and Roy, Anindya}, year={2008}, pages={5407–5420}, language={en} }

@inproceedings{Yuan_Yan_Sonka_Yang_2021, address={Montreal, QC, Canada}, title={Large-scale Robust Deep AUC Maximization: A New Surrogate Loss and Empirical Studies on Medical Image Classification}, rights={https://doi.org/10.15223/policy-029}, ISBN={978-1-6654-2812-5}, url={https://ieeexplore.ieee.org/document/9710509/}, DOI={10.1109/ICCV48922.2021.00303}, abstractNote={Deep AUC Maximization (DAM) is a new paradigm for learning a deep neural network by maximizing the AUC score of the model on a dataset. Most previous works of AUC maximization focus on the perspective of optimization by designing eﬃcient stochastic algorithms, and studies on generalization performance of large-scale DAM on diﬃcult tasks are missing. In this work, we aim to make DAM more practical for interesting real-world applications (e.g., medical image classiﬁcation). First, we propose a new margin-based min-max surrogate loss function for the AUC score (named as the AUC min-maxmargin loss or simply AUC margin loss for short). It is more robust than the commonly used AUC square loss, while enjoying the same advantage in terms of large-scale stochastic optimization. Second, we conduct extensive empirical studies of our DAM method on four diﬃcult medical image classiﬁcation tasks, namely (i) classiﬁcation of chest x-ray images for identifying many threatening diseases, (ii) classiﬁcation of images of skin lesions for identifying melanoma, (iii) classiﬁcation of mammogram for breast cancer screening, and (iv) classiﬁcation of microscopic images for identifying tumor tissue. Our studies demonstrate that the proposed DAM method improves the performance of optimizing crossentropy loss by a large margin, and also achieves better performance than optimizing the existing AUC square loss on these medical image classiﬁcation tasks. Speciﬁcally, our DAM method has achieved the 1st place on Stanford CheXpert competition on Aug. 31, 2020. To the best of our knowledge, this is the ﬁrst work that makes DAM succeed on large-scale medical image datasets. We also conduct extensive ablation studies to demonstrate the advantages of the new AUC margin loss over the AUC square loss on benchmark datasets. The proposed method is implemented in our open-sourced library LibAUC (www.libauc. org) whose github address is https://github.com/Optimization-AI/LibAUC.}, booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, publisher={IEEE}, author={Yuan, Zhuoning and Yan, Yan and Sonka, Milan and Yang, Tianbao}, year={2021}, month=oct, pages={3020–3029}, language={en} }

@article{Zhang_Xu_2022, title={AUC optimization for deep learning-based voice activity detection}, volume={2022}, ISSN={1687-4722}, DOI={10.1186/s13636-022-00260-9}, abstractNote={Voice activity detection (VAD) based on deep neural networks (DNN) have demonstrated good performance in adverse acoustic environments. Current DNN-based VAD optimizes a surrogate function, e.g., minimum cross-entropy or minimum squared error, at a given decision threshold. However, VAD usually works on-the-fly with a dynamic decision threshold, and the receiver operating characteristic (ROC) curve is a global evaluation metric for VAD at all possible decision thresholds. In this paper, we propose to maximize the area under the ROC curve (MaxAUC) by DNN, which can maximize the performance of VAD in terms of the entire ROC curve. However, the objective of the AUC maximization is nondifferentiable. To overcome this difficulty, we relax the nondifferentiable loss function to two differentiable approximation functions—sigmoid loss and hinge loss. To study the effectiveness of the proposed MaxAUC-DNN VAD, we take either a standard feedforward neural network or a bidirectional long short-term memory network as the DNN model with either the state-of-the-art multi-resolution cochleagram or short-term Fourier transform as the acoustic feature. We conducted noise-independent training to all comparison methods. Experimental results show that taking AUC as the optimization objective results in higher performance than the common objectives of the minimum squared error and minimum cross-entropy. The experimental conclusion is consistent across different DNN structures, acoustic features, noise scenarios, training sets, and languages.}, number={1}, journal={EURASIP Journal on Audio, Speech, and Music Processing}, author={Zhang, Xiao-Lei and Xu, Menglong}, year={2022}, month=oct, pages={27}, language={en} }

@article{LeDell_etal_2016, title={AUC-Maximizing Ensembles through Metalearning}, volume={12}, ISSN={2194-573X}, DOI={10.1515/ijb-2015-0035}, abstractNote={Area Under the ROC Curve (AUC) is often used to measure the performance of an estimator in binary classification problems. An AUC-maximizing classifier can have significant advantages in cases where ranking correctness is valued or if the outcome is rare. In a Super Learner ensemble, maximization of the AUC can be achieved by the use of an AUC-maximining metalearning algorithm. We discuss an implementation of an AUC-maximization technique that is formulated as a nonlinear optimization problem. We also evaluate the effectiveness of a large number of different nonlinear optimization algorithms to maximize the cross-validated AUC of the ensemble fit. The results provide evidence that AUC-maximizing metalearners can, and often do, out-perform non-AUC-maximizing metalearning methods, with respect to ensemble AUC. The results also demonstrate that as the level of imbalance in the training data increases, the Super Learner ensemble outperforms the top base algorithm by a larger degree.}, number={1}, journal={The international journal of biostatistics}, author={LeDell, Erin and van der Laan, Mark J. and Peterson, Maya}, year={2016}, month=may, pages={203–218} }

@inproceedings{Zhou_Ying_Skiena_2020, title={Online AUC Optimization for Sparse High-Dimensional Datasets}, ISSN={2374-8486}, url={https://ieeexplore.ieee.org/abstract/document/9338419}, DOI={10.1109/ICDM50108.2020.00097}, abstractNote={The Area Under the ROC Curve (AUC) is a widely used performance measure for imbalanced classification arising from many application domains where high-dimensional sparse data is abundant. In such cases, each d dimensional sample has only k non-zero features with k << d, and data arrives sequentially in a streaming form. Current online AUC optimization algorithms have high per-iteration cost O(d) and usually produce non-sparse solutions in general, and hence are not suitable for handling the data challenge mentioned above. In this paper, we aim to directly optimize the AUC score for high-dimensional sparse datasets under online learning setting and propose a new algorithm, FTRL-AUC. Our proposed algorithm can process data in an online fashion with a much cheaper per-iteration cost O(k), making it amenable for high-dimensional sparse streaming data analysis. Our new algorithmic design critically depends on a novel reformulation of the U-statistics AUC objective function as the empirical saddle point reformulation, and the innovative introduction of the “lazy update” rule so that the per-iteration complexity is dramatically reduced from O(d) to O(k). Furthermore, FTRL-AUC can inherently capture sparsity more effectively by applying a generalized Follow-The-Regularized-Leader (FTRL) framework. Experiments on real-world datasets demonstrate that FTRL-AUC significantly improves both run time and model sparsity while achieving competitive AUC scores compared with the state-of-the-art methods. Comparison with the online learning method for logistic loss demonstrates that FTRL-AUC achieves higher AUC scores especially when datasets are imbalanced.}, booktitle={2020 IEEE International Conference on Data Mining (ICDM)}, author={Zhou, Baojian and Ying, Yiming and Skiena, Steven}, year={2020}, month=nov, pages={881–890} }

@inproceedings{Calders_Jaroszewicz_2007, address={Berlin, Heidelberg}, series={Lecture Notes in Computer Science}, title={Efficient AUC Optimization for Classification}, ISBN={978-3-540-74976-9}, DOI={10.1007/978-3-540-74976-9_8}, abstractNote={In this paper we show an efficient method for inducing classifiers that directly optimize the area under the ROC curve. Recently, AUC gained importance in the classification community as a mean to compare the performance of classifiers. Because most classification methods do not optimize this measure directly, several classification learning methods are emerging that directly optimize the AUC. These methods, however, require many costly computations of the AUC, and hence, do not scale well to large datasets. In this paper, we develop a method to increase the efficiency of computing AUC based on a polynomial approximation of the AUC. As a proof of concept, the approximation is plugged into the construction of a scalable linear classifier that directly optimizes AUC using a gradient descent method. Experiments on real-life datasets show a high accuracy and efficiency of the polynomial approximation.}, booktitle={Knowledge Discovery in Databases: PKDD 2007}, publisher={Springer}, author={Calders, Toon and Jaroszewicz, Szymon}, editor={Kok, Joost N. and Koronacki, Jacek and Lopez de Mantaras, Ramon and Matwin, Stan and Mladenič, Dunja and Skowron, Andrzej}, year={2007}, pages={42–53}, collection={Lecture Notes in Computer Science}, language={en} }

@inproceedings{Yang_Zhou_Lei_Ying_2020, title={Stochastic Hard Thresholding Algorithms for AUC Maximization}, ISSN={2374-8486}, url={https://ieeexplore.ieee.org/abstract/document/9338276}, DOI={10.1109/ICDM50108.2020.00083}, abstractNote={In this paper, we aim to develop stochastic hard thresholding algorithms for the important problem of AUC maximization in imbalanced classification. The main challenge is the pairwise loss involved in AUC maximization. We overcome this obstacle by reformulating the U-statistics objective function as an empirical risk minimization (ERM), from which a stochastic hard thresholding algorithm (SHT-AUC) is developed. To our best knowledge, this is the first attempt to provide stochastic hard thresholding algorithms for AUC maximization with a per-iteration cost O(bd) where d and b are the dimension of the data and the minibatch size, respectively. We show that the proposed algorithm enjoys the linear convergence rate up to a tolerance error. In particular, we show, if the data is generated from the Gaussian distribution, then its convergence becomes slower as the data gets more imbalanced. We conduct extensive experiments to show the efficiency and effectiveness of the proposed algorithms.}, booktitle={2020 IEEE International Conference on Data Mining (ICDM)}, author={Yang, Zhenhuan and Zhou, Baojian and Lei, Yunwen and Ying, Yiming}, year={2020}, month=nov, pages={741–750} }

@article{Gajowniczek_Zabkowski_2021, title={ImbTreeAUC: An R package for building classification trees using the area under the ROC curve (AUC) on imbalanced datasets}, volume={15}, ISSN={2352-7110}, DOI={10.1016/j.softx.2021.100755}, abstractNote={In this paper, we propose a novel R package, named ImbTreeAUC, for building binary and multiclass decision tree using the area under the receiver operating characteristic (ROC) curve. The package provides nonstandard measures to select an optimal split point for an attribute as well as the optimal attribute for splitting through the application of local, semiglobal and global AUC measures. Additionally, ImbTreeAUC can handle imbalanced data, which is a challenging issue in many practical applications. The package supports cost-sensitive learning by defining a misclassification cost matrix and weight-sensitive learning. It accepts all types of attributes, including continuous, ordered and nominal attributes. The package and its code are made freely available.}, journal={SoftwareX}, author={Gajowniczek, Krzysztof and Ząbkowski, Tomasz}, year={2021}, month=july, pages={100755} }

@inproceedings{Ataman_Street_Zhang_2006, title={Learning to Rank by Maximizing AUC with Linear Programming}, ISSN={2161-4407}, url={https://ieeexplore.ieee.org/document/1716080}, DOI={10.1109/IJCNN.2006.246669}, abstractNote={Area Under the ROC Curve (AUC) is often used to evaluate ranking performance in binary classification problems. Several researchers have approached AUC optimization by approximating the equivalent Wicoxon-Mann-Whitney (WMW) statistic. We present a linear programming approach similar to 1-norm Support Vector Machines (SVMs) for instance ranking by an approximation to the WMW statistic. Our formulation can be applied to nonlinear problems by using a kernel function. Our ranking algorithm outperforms SVMs in both AUC and classification performance when using RBF kernels, but curiously not with polynomial kernels. We experiment with variations of chunking to handle the quadratic growth of the number of constraints in our formulation.}, booktitle={The 2006 IEEE International Joint Conference on Neural Network Proceedings}, author={Ataman, Kaan and Street, W. Nick and Zhang, Yi}, year={2006}, month=july, pages={123–129} }

@article{Su_Du_Wang_Wei_Liu_2022, title={Multi-variable AUC for sifting complementary features and its biomedical application}, volume={23}, ISSN={1477-4054}, DOI={10.1093/bib/bbac029}, abstractNote={Although sifting functional genes has been discussed for years, traditional selection methods tend to be ineffective in capturing potential specific genes. First, typical methods focus on finding features (genes) relevant to class while irrelevant to each other. However, the features that can offer rich discriminative information are more likely to be the complementary ones. Next, almost all existing methods assess feature relations in pairs, yielding an inaccurate local estimation and lacking a global exploration. In this paper, we introduce multi-variable Area Under the receiver operating characteristic Curve (AUC) to globally evaluate the complementarity among features by employing Area Above the receiver operating characteristic Curve (AAC). Due to AAC, the class-relevant information newly provided by a candidate feature and that preserved by the selected features can be achieved beyond pairwise computation. Furthermore, we propose an AAC-based feature selection algorithm, named Multi-variable AUC-based Combined Features Complementarity, to screen discriminative complementary feature combinations. Extensive experiments on public datasets demonstrate the effectiveness of the proposed approach. Besides, we provide a gene set about prostate cancer and discuss its potential biological significance from the machine learning aspect and based on the existing biomedical findings of some individual genes.}, number={2}, journal={Briefings in Bioinformatics}, author={Su, Yue and Du, Keyu and Wang, Jun and Wei, Jin-mao and Liu, Jian}, year={2022}, month=mar, pages={bbac029} }

@article{Sun_Wang_Wei_2017, title={AVC: Selecting discriminative features on basis of AUC by maximizing variable complementarity}, volume={18}, ISSN={1471-2105}, DOI={10.1186/s12859-017-1468-4}, abstractNote={The Receiver Operator Characteristic (ROC) curve is well-known in evaluating classification performance in biomedical field. Owing to its superiority in dealing with imbalanced and cost-sensitive data, the ROC curve has been exploited as a popular metric to evaluate and find out disease-related genes (features). The existing ROC-based feature selection approaches are simple and effective in evaluating individual features. However, these approaches may fail to find real target feature subset due to their lack of effective means to reduce the redundancy between features, which is essential in machine learning.}, number={3}, journal={BMC Bioinformatics}, author={Sun, Lei and Wang, Jun and Wei, Jinmao}, year={2017}, month=mar, pages={50}, language={en} }

@article{Kim_Shin_2026, title={Variable selection in AUC-optimizing classification}, volume={213}, ISSN={0167-9473}, DOI={10.1016/j.csda.2025.108256}, abstractNote={Optimizing the receiver operating characteristic (ROC) curve is a popular way to evaluate a binary classifier under imbalanced scenarios frequently encountered in practice. A practical approach to constructing a linear binary classifier is presented by simultaneously optimizing the area under the ROC curve (AUC) and selecting informative variables in high dimensions. In particular, the smoothly clipped absolute deviation (SCAD) penalty is employed, and its oracle property is established, which enables the development of a consistent BIC-type information criterion that greatly facilitates the tuning procedure. Both simulated and real data analyses demonstrate the promising performance of the proposed method in terms of AUC optimization and variable selection.}, journal={Computational Statistics & Data Analysis}, author={Kim, Hyungwoo and Shin, Seung Jun}, year={2026}, month=jan, pages={108256} }

@article{Xu_Suzuki_2014, title={Max-AUC Feature Selection in Computer-Aided Detection of Polyps in CT Colonography}, volume={18}, ISSN={2168-2208}, DOI={10.1109/JBHI.2013.2278023}, abstractNote={We propose a feature selection method based on a sequential forward floating selection (SFFS) procedure to improve the performance of a classifier in computerized detection of polyps in CT colonography (CTC). The feature selection method is coupled with a nonlinear support vector machine (SVM) classifier. Unlike the conventional linear method based on Wilks’ lambda, the proposed method selected the most relevant features that would maximize the area under the receiver operating characteristic curve (AUC), which directly maximizes classification performance, evaluated based on AUC value, in the computer-aided detection (CADe) scheme. We presented two variants of the proposed method with different stopping criteria used in the SFFS procedure. The first variant searched all feature combinations allowed in the SFFS procedure and selected the subsets that maximize the AUC values. The second variant performed a statistical test at each step during the SFFS procedure, and it was terminated if the increase in the AUC value was not statistically significant. The advantage of the second variant is its lower computational cost. To test the performance of the proposed method, we compared it against the popular stepwise feature selection method based on Wilks’ lambda for a colonic-polyp database (25 polyps and 2624 nonpolyps). We extracted 75 morphologic, gray-level-based, and texture features from the segmented lesion candidate regions. The two variants of the proposed feature selection method chose 29 and 7 features, respectively. Two SVM classifiers trained with these selected features yielded a 96% by-polyp sensitivity at false-positive (FP) rates of 4.1 and 6.5 per patient, respectively. Experiments showed a significant improvement in the performance of the classifier with the proposed feature selection method over that with the popular stepwise feature selection based on Wilks’ lambda that yielded 18.0 FPs per patient at the same sensitivity level.}, number={2}, journal={IEEE Journal of Biomedical and Health Informatics}, author={Xu, Jian-Wu and Suzuki, Kenji}, year={2014}, month=mar, pages={585–593} }

@inproceedings{Ribeiro_2021, title={Feature Selection Method AUC-Based with Estimation Probability and Smoothing}, ISSN={2409-2983}, url={https://ieeexplore.ieee.org/abstract/document/9659733}, DOI={10.1109/ICEET53442.2021.9659733}, abstractNote={This work presents a new feature selection algorithm named Feature Selection Method AUC-Based with Estimation Probability and Smoothing. This method selects the best attributes of a dataset considering all possible values to each feature, associated to estimation probability and Laplace correction method (smoothing), which identifies variables with the highest AUC value. The experiments were carried out following two methodologies and compared with the results of other algorithms. This technique was evaluated with several medical databases, more specifically related to microarray gene expression. Results show the algorithm had results equivalent to the state of art approaches based on AUC.}, booktitle={2021 International Conference on Engineering and Emerging Technologies (ICEET)}, author={Ribeiro, Guilherme and Gonçalves, Cristhiane and Victor dos Santos, Paulo and Melgaço Barbosa, Rommel}, year={2021}, month=oct, pages={1–8} }

@book{Sheskin_2020, address={New York}, edition={5}, title={Handbook of Parametric and Nonparametric Statistical Procedures, Fifth Edition}, ISBN={978-0-429-18619-6}, DOI={10.1201/9780429186196}, abstractNote={Following in the footsteps of its bestselling predecessors, the Handbook of Parametric and Nonparametric Statistical Procedures, Fifth Edition provides researchers, teachers, and students with an all-inclusive reference on univariate, bivariate, and multivariate statistical procedures.New in the Fifth Edition:Substantial updates and new material th}, publisher={Chapman and Hall/CRC}, author={Sheskin, David J.}, year={2020}, month=june }
