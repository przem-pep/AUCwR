% !TeX root = RJwrapper.tex
\title{Area under ROC curve -- review and efficient computation in R}


\author{by Błażej Kochański, Przemysław Peplinski, Miriam Nieslona, Wiktor Galewski, and Piotr Geremek}

\maketitle

\abstract{%
The AUC (Area Under the Curve) measure is widely used in statistical classification and machine learning, including credit scoring, where it is employed to assess the quality of predictive models. The goal of this paper is to review methods for calculating the AUC measure, followed by an analysis of the efficiency of computing this measure in R.
}

\section{Introduction}\label{introduction}

Assessing the quality of predictive models is a critical stage in
machine learning and statistical inference processes. Among the many
available metrics, the Area Under the Receiver Operating Characteristic
curve (AUC) has established itself as a standard measure of
discrimination, particularly in binary classification problems. The
popularity of this metric stems from its independence from the chosen
cut-off point and its robustness to class imbalance, making it useful in
diverse fields such as medical diagnostics, psychology, and credit risk
assessment.

Modern statistical applications require not only methodological
correctness but also high computational efficiency. In the era of Big
Data and increasing complexity of validation procedures, a single
calculation of the AUC is often insufficient. Similar challenges are
posed by simulation market models in banking, where generating ROC
curves without historical data serves to estimate the impact of scoring
models on portfolio profitability. In such scenarios, classical
algorithm implementations can become a computational bottleneck,
rendering procedures like the bootstrap impractical.

The goal of this paper is to review methods for calculating the AUC
measure and to analyze the efficiency of available implementations
within the R environment. In the first part of the work, we organize
definitions and highlight the mathematical links between AUC and other
measures, such as the Gini coefficient. Next, we discuss the main
classes of algorithms used for AUC estimation: trapezoidal integration,
optimized pairwise comparisons, and rank-based approaches. Finally, we
present the results of performance benchmarks for popular R packages and
propose our alternatives, identifying optimal solutions for different
sizes of large datasets.

\section{ROC curve}\label{roc-curve}

The ROC curve is one of the most important tools for assessing the
effectiveness of binary classifiers. It was introduced in the 1940s in
the context of radar signal analysis, which is the origin of its name --
\emph{Receiver Operating Characteristic} \citep{Junge_Dettori_2024}. Today, the
ROC curve is used in fields such as medicine, biotechnology, computer
science, and banking, where it is used to evaluate scoring models.

\textbf{Definition}

The ROC curve is a graph created by ``plotting the qualitative
characteristics of binary classifiers generated from a model using many
different cut-off points'' \citep{Gromada_2016}, illustrating the model's
predictive effectiveness. Each point on the curve corresponds to TPR
(sensitivity) and FPR (1 -- specificity) values as a function of the
classifier's varying cut-off point \citep{Fawcett_2006}.

\textbf{Construction of the ROC Curve}

To construct an ROC curve, the threshold that determines whether the
classifier assigns an observation to the positive class is modified. For
each threshold, TPR and FPR are calculated. The results are presented on
a graph where the X-axis represents FPR and the Y-axis represents TPR.

Point (0,1) represents an ideal classifier: no false positives and full
detection of positive cases. A curve closer to this point is more
desirable \citep{Hanley_McNeil_1982} -- this is equivalent to a larger Area
Under the Curve, or AUC. The diagonal line from (0,0) to (1,1)
represents a random classifier (AUC = 0.5).

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{report_files/figure-latex/auc-gini-1} 

}

\caption{Geometric interpretation: AUC (AUROC) is the area under the ROC curve, Gini is twice the area between the diagonal y=x and the ROC curve.}\label{fig:auc-gini}
\end{figure}

\section{AUC -- alternative names and related measures}\label{auc-alternative-names-and-related-measures}

The Area Under the Receiver Operating Characteristic (ROC) curve,
commonly abbreviated as AUC, is a fundamental metric for evaluating the
performance of classification models. However, due to its independent
adoption and development across diverse fields, ranging from biomedical
statistics and psychology to econometrics and credit scoring, this
measure appears in the literature under numerous aliases and
mathematical formulations. Recognizing these equivalences is essential
for researchers to leverage efficient computational algorithms developed
in different domains.

\textbf{Alternative names and interpretations of AUC}

In medical statistics and epidemiology, particularly when evaluating
logistic regression models with binary outcomes, the AUC is frequently
referred to as the \emph{c-statistic} (or concordance statistic). This term
defined by Harrell et al. \citep{Harrell_Califf_Pryor_Lee_Rosati_1982} and
further elaborated by, for example Austin and Steyerberg
\citep{Austin_Steyerberg_2012}, represents the probability of concordance
between predicted probabilities and observed outcomes. It quantifies the
predictive accuracy of a model by estimating the probability that a
randomly selected subject who experienced the outcome has a higher
predicted probability than a randomly selected subject who did not.

This probabilistic interpretation serves as the bridge to non-parametric
statistics. Area under the ROC curve is statistically equivalent to the
Mann-Whitney U statistic divided by the product of the sample sizes of
the two groups, where in this context, the metric is often interpreted
as the \emph{probability of superiority} indicating the likelihood that a
randomly chosen positive case ranks higher than a randomly chosen
negative case \citep{Hanley_McNeil_1982}.

In the behavioral sciences and psychology, similar concepts have been
developed to measure effect sizes. There the AUC corresponds to the
\emph{Common Language Effect Size} (CL/CLES) \citep{McGraw_Wong_1992}, which
converts an effect size into a probability, specifically defined as the
probability that a score sampled at random from one distribution will be
greater than a score sampled from another. This metric was developed to
provide an index of effect size that is intuitively interpretable by
audiences without statistical training. While the original formulation
by McGraw and Wong focused on normal distributions, the later proposed
\emph{measure of stochastic superiority} (denoted as \(A\))
\citep{Vargha_Delaney_2000} generalized CLES concept so that it could be
applicable to any variable that is at least ordinally scaled, regardless
of the distribution form, and is identical to the CL statistic in the
continuous case. Furthermore, in the context of non-parametric analysis
for longitudinal data the \emph{relative treatment effect} was defined
\citep{Brunner_Munzel_Puri_2002}. This metric estimates the probability that
an observation from one group is smaller than an observation from
another, effectively serving as a probabilistic analog to the AUC
\citep{Brunner_Munzel_Puri_2002}, \citep{Engelmann_Hayden_Tasche_2003}.

\textbf{The Gini coefficient and other AUC related measures}

While the AUC scales from 0.5 (random classification) to 1.0 (perfect
classification), many disciplines prefer a measure scaled between 0 and
1 (or -1 and 1) to represent the strength of association or
discriminatory power. This leads to the Gini coefficient (or Gini
index). The Gini coefficient used in classification contexts is simply a
linear transformation of the AUC, defined by the relationship
\citep{Hand_Till_2001}: \[\text{Gini} = 2 \cdot \text{AUC} - 1\] Cliff {[}1993{]}
discusses an ordinal statistic, \(d\), which compares the number of times
a score from one group is higher than one from another versus the
reverse. This statistic of ordinal dominance, later named \emph{Cliff's
Delta} \citep{Bais_Van_Der_Neut_2022} was described as a non-parametric
effect size based on data observations, which quantifies the difference
between the probability that a value from one group is higher than a
value from the other and the reverse probability.

\textbf{Relation to the U statistic in the Mann-Whitney}

There is a direct relationship between AUC and the Mann-Whitney U
statistic.

AUC is equivalent to the probability that a randomly selected positive
case will have a higher score than a randomly selected negative case
\citep{Hanley_McNeil_1982, Bamber_1975}.
\[\text{AUC} = \frac{U}{n_1 \times n_0}\] where \(U\) is the number of
pairs in which the ``bad'' score is \textless{} the ``good'' score, \(n_1\) is the
number of ``bad'' scores, and \(n_0\) is the number of ``good'' scores.

This relationship provides the statistical basis for treating AUC as a
measure of discrimination and allows confidence intervals to be
constructed using Mann-Whitney statistics theory \citep{Hanley_McNeil_1982}.

\textbf{Relation to the Somers' D}

Somers' D coefficient is a measure of association between ordinal
variables, which also shows a direct relationship with AUC. It can be
considered a generalization of AUC for ordinal variables \citep{Newson_2002}.

Somers' D is defined based on concordant and discordant pairs. A pair of
observations (i,j) is concordant if the ranking of the independent
variable X and the ranking of the dependent variable Y are in the same
order, i.e., if (\(X_i - X_j\)) and (\(Y_i - Y_j\)) have the same sign. A
pair is discordant if the signs are opposite. Somers' D is calculated as
the difference between the number of concordant and discordant pairs,
divided by the number of unrelated pairs on the independent variable X
\citep{Somers_1962}.

In binary classification: \[\text{Somers' D} = 2 \cdot \text{AUC} - 1\]
which links it directly to AUC \citep{Newson_2002}.

\textbf{Cumulative accuracy profile}

The CAP curve is a graphical tool used to evaluate the performance of
classification models, primarily in the area of creditworthiness
assessment. It shows the cumulative percentage of positive cases
relative to the cumulative percentage of the entire population, sorted
by predicted probability of default \citep{Engelmann_Hayden_Tasche_2003}.

Accuracy Ratio (AR): \[\text{AR} = \frac{A}{B}\] where \(A\) is the area
between the model CAP and the random CAP, and \(B\) is the area between
the ideal CAP and the random CAP. Areas A and B are calculated using the
trapezoidal method \citep{Engelmann_Hayden_Tasche_2003}.

Within the credit scoring literature, the AR metric is sometimes
referred to as the \emph{pseudo Gini} index to differentiate it from the
classical Gini index used in economics to measure inequality. It was
established that the pseudo Gini index is based on the concentration
curve (CAP) and that its value is always equal to the Accuracy Ratio
(referred to as Accuracy Rate) for any scoring model \citep{Idczak_2019}.

The relationship between AUC and the CAP curve can be described by the
following formula \citep{Engelmann_Hayden_Tasche_2003}:
\[\text{AUC} = \frac{\text{AR} + 1}{2}\]

\section{The need for computational efficiency in AUC estimation}\label{the-need-for-computational-efficiency-in-auc-estimation}

In the context of modern statistical modeling and machine learning, a
single calculation of the AUC metric is often insufficient for a
comprehensive assessment of model quality. Complex validation,
optimization, and simulation procedures require the AUC statistic to be
computed repeatedly, imposing high demands on the computational
efficiency of the underlying algorithms. The following sections outline
key areas where the performance of AUC estimation is critical.

\textbf{Uncertainty estimation and permutation tests}

The estimation of confidence intervals and the verification of
statistical hypotheses for the AUC often rely on resampling methods,
such as bootstrapping or permutation tests. While necessary when simple
parametric assumptions cannot be met, these methods generate significant
computational loads.

In the case of massive datasets, the process of generating a single
performance estimate can be computationally expensive
\citep{LeDell_Petersen_Van_Der_Laan_2015}. The authors point out that when
using complex prediction methods, even with relatively small datasets,
cross-validation can consume a large amount of time, making the
bootstrap a computationally intractable approach to variance estimation
in many practical settings.

Similarly, in the context of meta-analysis of diagnostic accuracy
studies, there are instances of bootstrap algorithms usage to determine
confidence intervals for the AUC of the Summary ROC (SROC) curve
\citep{Noma_Matsushima_Ishii_2021}. Their approach involves computing AUC
estimates from a large number of bootstrap samples (e.g., \(B=1000\)),
which necessitates efficient calculation routines.

Permutation-based inference also demands repeated calculation. Bandos,
Rockette, and Gur (2006) developed a permutation test for comparing ROC
curves in multireader studies. An exact permutation test in this setting
is formed by determining the frequency of the statistic---estimating the
average difference in AUCs across all possible exchanges of reader
ratings. For larger samples, this requires an asymptotic approach due to
the computational intensity of calculating differences for every
permutation \citep{Bandos_Rockette_Gur_2006}. Furthermore, Pauly, Asendorf,
and Konietschke (2016) proposed rank-based studentized permutation
methods for the nonparametric Behrens-Fisher problem, which corresponds
to inference for the AUC. They demonstrated that the studentized
permutation distribution ofthe Brunner-Munzel rank statistic is
asymptotically standard normal, providing a theoretical foundation for
consistent confidence intervals and tests, which rely on these intensive
permutation procedures \citep{Pauly_Asendorf_Konietschke_2016}.

\textbf{Variable importance measures}

Efficient AUC computation is a prerequisite for specific Variable
Importance Measures (VIM). Therefore an AUC-based permutation variable
importance measure for Random Forests was introduced, designed to be
more robust to unbalanced classes than standard error-rate-based
measures. This procedure requires the AUC to be computed for each tree
in the forest, both before and after permuting the values of a given
predictor. Given the number of trees in a forest and the number of
predictors, this approach results in a vast number of AUC calculations,
far exceeding the computational cost of standard single-pass metrics
\citep{Janitza_Strobl_Boulesteix_2013}.

Beyond model-specific methods, model-agnostic interpretability
frameworks also rely heavily on repeated metric estimation. The DALEX
package \citep{Biecek_2018} implements a permutation-based variable
importance method applicable to any predictive model. This approach
measures the change in model performance, such as the drop in AUC
(represented as the loss function \(L(y, \hat{y}) = 1 - AUC\)), after
permuting the values of a single predictor. To ensure stability of the
importance estimates, this permutation process is typically repeated \(B\)
times (e.g., \(B=10\)) for every feature in the dataset. Consequently, for
a model with \(p\) features, assessing global feature importance requires
\(p \times B\) independent AUC calculations, creating a linear dependency
between the number of features and the computational cost.

\textbf{AUC maximization algorithms}

In the era of Big Data, learning algorithms that directly maximize AUC,
rather than accuracy, are gaining importance but algorithms maximizing
model accuracy do not necessarily maximize the AUC score. However,
direct AUC maximization presents a computational challenge because the
function is non-decomposable over individual examples. This has led to
the development of stochastic AUC maximization methods for big data and
``Deep AUC Maximization'' (DAM) for deep learning, where optimization must
be performed efficiently on large-scale datasets \citep{Yang_Ying_2023}.

\textbf{Simulation and curve modeling in credit risk}

In credit risk management, theoretical ROC curve models are employed to
simulate and assess the impact of scoring models when actual data is
unavailable or limited. Kochański (2022) notes that fitting models such
as the binormal or bigamma curves is helpful for ``generating ROC curves
without underlying data''. This capability is essential for assessing the
impact of a credit scorecard that is ``yet to be built''
\citep{Kochański_2022}. Such simulation analyses often involve generating
numerous curves and estimating their parameters to forecast portfolio
quality, further justifying the need for computationally efficient AUC
estimation methods.

Beyond single-curve fitting, Kochański (2021) extends the analysis to a
dynamic market environment, proposing a simulation model for risk and
pricing competition. This framework explores how the discrimination
power of credit scoring models influences key business metrics,
illustrating the ``trade-off between profitability, market share, and
credit loss rates'' \citep{Kochanski_2021}. The study demonstrates that even
marginal improvements in discrimination power can yield substantial
benefits, a conclusion derived from simulation scenarios that model the
interactions between lenders and borrowers. Such comprehensive market
analyses require processing numerous scenarios to identify
profit-maximizing strategies, further highlighting the role of
performance metrics as fundamental parameters in complex economic
models.

\section{R packages}\label{r-packages}

\begin{table}[H]
\centering
\caption{\label{tab:unnamed-chunk-1}Functions computing AUC.}
\centering
\resizebox{\ifdim\width>\linewidth\linewidth\else\width\fi}{!}{
\begin{tabular}[t]{ll>{}lll}
\toprule
Package & Function & Usage & Method & Language\\
\midrule
bigstatsr & AUC & \ttfamily{AUC(-pred, target)} & Pairwise comparison (optimized) & C++, R\\
\addlinespace
caTools & colAUC & \ttfamily{colAUC(-pred, target)[1, 1]} & Rank sum & R\\
\addlinespace
cvAUC & AUC & \ttfamily{AUC(-pred, target)} & Trapezoidal rule & R\\
\addlinespace
DescTools & Cstat & \ttfamily{Cstat(-pred, target)} & Pairwise comparison (optimized) & C++, R\\
\addlinespace
effsize & VD.A & \ttfamily{VD.A(pred \textasciitilde{} factor(target))\$estimate} & Rank sum & R\\
\addlinespace
fbroc & boot.roc & \ttfamily{boot.roc(-pred, as.logical(target))\$auc} & Trapezoidal rule & C++, R\\
\addlinespace
Hmisc & somers2 & \ttfamily{somers2(-pred, target)["C"]} & Rank sum & R\\
\addlinespace
MLmetrics & AUC & \ttfamily{AUC(-pred, target)} & Rank sum & R\\
\addlinespace
mltools & auc\_roc & \ttfamily{auc\_roc(-pred, target)} & Trapezoidal rule & R\\
\addlinespace
ModelMetrics & auc & \ttfamily{auc(target, -pred)} & Rank sum & C++, R\\
\addlinespace
precrec & evalmod & \ttfamily{evalmod(mmdata(scores = -pred, labels = target), mode="aucroc")\$uaucs\$aucs} & Rank sum & C++, R\\
\addlinespace
pROC & auc & \ttfamily{auc(target, pred, lev=c('0', '1'), dir=">")} & Trapezoidal rule & R\\
\addlinespace
rcompanion & vda & \ttfamily{vda(x = pred[target == 0], y = pred[target == 1], digits=100)} & Rank sum & R\\
\addlinespace
ROCR & performance & \ttfamily{performance(prediction(-pred, target), "auc")@y.values[[1]]} & Trapezoidal rule & R\\
\addlinespace
scikit-learn & roc\_auc\_score & \ttfamily{import("sklearn.metrics")\$roc\_auc\_score(target, -pred)} & Trapezoidal rule & Python, C\\
\addlinespace
scorecard & perf\_eva & \ttfamily{perf\_eva(-pred, target, binomial\_metric = "auc", show\_plot=FALSE)\$binomial\_metric\$dat\$AUC} & Trapezoidal rule & R\\
\addlinespace
yardstick & roc\_auc\_vec & \ttfamily{roc\_auc\_vec(as.factor(target), pred)} & Trapezoidal rule & R\\
\addlinespace
DescTools & SomersDelta & \ttfamily{SomersDelta(-pred, target)/2 + 1/2} & Pairwise comparison (unoptimized) & C++, R\\
\bottomrule
\end{tabular}}
\end{table}

We identified three main types of algorithms for AUC computation: (1)
trapezoidal integration over the ROC Curve, (2) (optimized) pairwise
comparison, (3) and rank-based (Mann--Whitney U statistic formulation).

Let \((s_i, y_i)\) for \(i = 1, \dots, n\) denote the score assigned to an
account and its corresponding true label. In line with standard practice
in credit scoring, we assume that \(y_i=1\) indicates a ``bad'' account
(e.g., one that defaults, doesn't repay the loan), while \(y_i=0\)
represents a good account. A properly functioning scoring should assign
lower scores to accounts with a higher predicted probability of being
bad, and higher scores to those likely to be good.

Let \(n_1 = \sum_{i=1}^{n}\mathbb{I}(y_i=1)\) denote the number of bad
accounts, and \(n_0 = \sum_{i=1}^{n}\mathbb{I}(y_i=0)\) -- number of good
accounts.

\textbf{Trapezoidal integration over the ROC curve}

\[\text{AUC} = \sum_{k=1}^{m-1}(\text{FPR}_{k+1} - \text{FPR}_{k})\cdot\frac{\text{TPR}_{k+1}+\text{TPR}_k}{2}\]

where \(m\) is the number of distinct score thresholds from lowest to the
highest score, \(TPR_k\) is the True Positive Rate, \(FPR_k\) is the False
Positive Rate. \(TPR_k=TP_k/n_1\) and \(FPR_k=FP_k/n_0\), where \(TP_k\) is
the number of true positives, \(\text{FP}_k\) is the number of false
positives, \(n_1\) is the number of positive cases, \(n_0\) is the number of
negative cases.

\textbf{Optimized pairwise comparison}

AUC as the probability that a randomly chosen positive instance receives
a higher score than a randomly chosen negative instance:

\[\text{AUC} = \frac{1}{n_1 n_0}\sum_{i:y_i=1}\sum_{j:y_j=0}\left[\mathbb{I}(s_i<s_j)+\frac{1}{2}\mathbb{I}(s_i=s_j)\right]\]

\textbf{Rank-based (Mann--Whitney U statistic formulation)}

\[\text{AUC} = \frac{\bar{R}_{1}-{n_1(n_1+1)}/2}{n_0}\]

where \(\bar{R}_{1}\) is the mean rank for \(s_i\) where \(y_i=1\):

\[\bar{R}_{1} = \frac{1}{n_1}\sum_{i:y_i=1}\text{Rank}(s_i)\]

\section{Efficiency study}\label{efficiency-study}

\subsection{Benchmarking packages}\label{benchmarking-packages}

All of the R packages that can be used to calculate the AUC measure have
had their performance speed tested. In every benchmark, each function
was called 100 times, and there are a total of three benchmarks, each
with a different number of observations. The predictor \emph{pred} has been
generated from the normal distribution, with the first half of the
observations using \(N(0, 1)\), and the second half being \(N(1, 1)\). The
\emph{target} variable is a vector, where the first half of values are all
equal to 1 and the second half of values are all equal to 0.

\textbf{First benchmark - 1000 observations}

\includegraphics{report_files/figure-latex/unnamed-chunk-2-1.pdf}

\textbf{Second benchmark - 10000 observations}

\includegraphics{report_files/figure-latex/unnamed-chunk-3-1.pdf}

\textbf{Third benchmark - 100000 observations}

\includegraphics{report_files/figure-latex/unnamed-chunk-4-1.pdf}

The time needed to complete the calculations varies greatly and spans
multiple orders of magnitude. For example, using \emph{bigstatsr} to
calculate the AUC metrics is almost 100 times faster than by using the
\emph{scorecard} package. There may be different causes to this, for example
a function might additionally perform other calculations, like
generating confidence intervals or also calculating other metrics, or
otherwise due to the algorithms used being inefficient, or perhaps even
both of these issues at once.

In general, \emph{bigstatsr} offers the fastest way of computation overall.
While \emph{MLMetrics} and \emph{Hmisc} are slightly faster on the smallest data
set, \emph{bigstatsr} noticeably outperforms them as the number of
observations increases.

It is therefore recommended to use the \emph{AUC} function from the
\emph{bigstatsr} package whenever calculating the AUC metric in R, whenever
an already existing function is desired.

\subsection{Benchmarking the algorithms}\label{benchmarking-the-algorithms}

The three identified algorithms have been tested against each other, by
creating as optimised functions implementing them as possible. They only
contain the calculation, with no checks whether the inputs are correct,
to minimise potential interference and isolate the algorithms. They have
been performed on the same observations as the package benchmarks.

\textbf{First benchmark - 1000 observations}

\includegraphics{report_files/figure-latex/unnamed-chunk-5-1.pdf}

\textbf{Second benchmark - 10000 observations}

\includegraphics{report_files/figure-latex/unnamed-chunk-6-1.pdf}

\textbf{Third benchmark - 100000 observations}

\includegraphics{report_files/figure-latex/unnamed-chunk-7-1.pdf}

In all the benchmarks, the optimised pairwise comparison method is the
fastest, with the trapezoid method being only slightly slower. The
rank-sum method becomes noticeably slower as the number of observations
increases. As a result, whenever developing a new function to calculate
the AUC metric, it is recommended to implement it using the pairwise
comparison method.

\bibliography{RJreferences.bib}

\address{%
Błażej Kochański\\
\\%
\\
%
%
%
%
}

\address{%
Przemysław Peplinski\\
Gdańsk University of Technology\\%
Faculty of Management and Economics\\ Gdańsk\\
%
%
%
%
}

\address{%
Miriam Nieslona\\
\\%
\\
%
%
%
%
}

\address{%
Wiktor Galewski\\
\\%
\\
%
%
%
%
}

\address{%
Piotr Geremek\\
\\%
\\
%
%
%
%
}
