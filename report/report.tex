% !TeX root = RJwrapper.tex
\title{Area under ROC curve -- review and efficient computation in R}


\author{by Błażej Kochański, Przemysław Peplinski, Miriam Nieslona, Wiktor Galewski, and Piotr Geremek}

\maketitle

\abstract{%
The Area Under the Receiver Operating Characteristic Curve (AUC) is a widely used measure for evaluating the performance of binary classification models. In the literature and in practice, it appears under various names and is closely related to other performance measures. In this paper, we review these formulations and discuss the motivation for efficient AUC computation in empirical analysis. We survey R packages that provide implementations of AUC and describe the algorithms they employ. We then conduct a benchmarking study to compare the execution time of selected implementations. \textbf{A case study illustrates how the choice of AUC computation method can substantially reduce computation time and accelerate analytical workflows.}
}

\section{Introduction}\label{introduction}

Assessing the quality of predictive models is a critical stage in
machine learning and statistical inference processes. Among the many
available metrics, the Area Under the Receiver Operating Characteristic
curve (AUC) has established itself as a standard measure of
discrimination, particularly in binary classification problems. The
popularity of this metric stems from its independence from the chosen
cut-off point and its robustness to class imbalance, making it useful in
diverse fields such as medical diagnostics, psychology, and credit risk
assessment.

Modern statistical applications require not only methodological
correctness but also high computational efficiency. In the era of Big
Data and increasing complexity of validation procedures, a single
calculation of the AUC is often insufficient. Similar challenges are
posed by simulation market models in banking, where generating ROC
curves without historical data serves to estimate the impact of scoring
models on portfolio profitability. In such scenarios, classical
algorithm implementations can become a computational bottleneck,
rendering procedures like the bootstrap impractical.

The goal of this paper is to review methods for calculating the AUC
measure and to analyze the efficiency of available implementations
within the R environment. In the first part of the work, we organize
definitions and highlight the mathematical links between AUC and other
measures, such as the Gini coefficient. Next, we discuss the main
classes of algorithms used for AUC estimation: trapezoidal integration,
optimized pairwise comparisons, and rank-based approaches. Finally, we
present the results of performance benchmarks for popular R packages and
propose our alternatives, identifying optimal solutions for different
sizes of large datasets.

\section{ROC curve and AUC}\label{roc-curve-and-auc}

The ROC curve is one of the most widely used tools for assessing the performance of binary classifiers. Introduced in the 1940s for radar signal analysis, the ROC curve derives its name from ``Receiver Operating Characteristic'' \citep{Junge_Dettori_2018}. Today, ROC curves are used across diverse fields---such as medicine, biotechnology, computer science, and banking---to evaluate scoring and classification models.

The ROC curve is plotted within the unit square, connecting points defined by sensitivity and specificity at various decision thresholds (``cut-offs''). In an ROC curve, the x-axis represents the false positive rate (FPR, or ``1 - specificity''), the proportion of false positives among all actual negatives. The y-axis represents sensitivity (or ``recall''), the proportion of correctly identified positives among all actual positives (true positive rate, TPR) \citep{Fawcett_2006}.

The AUC (Area Under the ROC Curve) provides a single-number summary of the classification performance of binary models across all possible decision thresholds. A ROC curve that passes through the point (0, 1) corresponds to an ideal classifier, achieving zero false positives and perfect detection of positive cases. Curves lying closer to this point are therefore more desirable \citep{Hanley_McNeil_1982}; they correspond to larger values of the Area Under the Curve (AUC). By contrast, the diagonal line connecting (0, 0) and (1, 1) represents a random classifier, for which the expected value of AUC equals 0.5.

In many application domains---most notably in credit scoring---the AUC is often transformed into a measure that tends to zero for a completely random classifier and equals one for a perfect classifier (it may also take negative values if the classifier's predictions are systematically reversed). This transformation (\(2\times\text{AUC} - 1\)) is commonly referred to as the Gini coefficient in the credit scoring literature. As we will see later, the same measure appears under different names in other fields.

Figure \ref{fig:auc-gini} illustrates an empirical ROC curve defined by eight discrete cut-off points, along with the areas corresponding to the AUC and the Gini coefficient.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{report_files/figure-latex/auc-gini-1} 

}

\caption{Geometric interpretation. The black piecewise linear curve represents the receiver operating characteristic (ROC) curve. The AUC (also referred to as AUROC, ROC score, C-statistic, or Vargha–Delaney A) is defined as the area under the ROC curve. A linear transformation of the AUC—known, among others, as the Gini coefficient, Cliff’s delta, Somers’ D, Accuracy Ratio, or the ROC skill score—equals twice the area between the diagonal line y=x and the ROC curve. }\label{fig:auc-gini}
\end{figure}

\section{AUC -- alternative names and related measures}\label{auc-alternative-names-and-related-measures}

The Area Under the Receiver Operating Characteristic (ROC) curve appears in the literature under numerous aliases and mathematical formulations. Recognizing the equivalence of these measures is essential for correct interpretation, comparison of results across disciplines, and informed use of software implementations. As shown below, the AUC is also referred to as the \emph{AUROC}, \emph{ROC score}, \emph{concordance statistic} (\emph{C-statistic}), and the \emph{Vargha-Delaney A statistic}. The AUC is closely related to the Mann-Whitney \emph{U} statistic and is used as a measure of effect size in the context of the Mann-Whitney test and related procedures, such as the Brunner-Munzel test. In this context, it is often referred to as the \emph{probability of superiority}, \emph{relative treatment effect} or a \emph{measure of stochastic superiority}.

The Gini coefficient, which is a simple linear transformation of the AUC, is also known as \emph{Cliff's delta}, the \emph{ROC skill score} (ROCSS), or the \emph{Accuracy Ratio}. It is a special case of \emph{Somers' D statistic} and is occasionally viewed as a variant of the \emph{rank-biserial correlation}.

In medical statistics and epidemiology, particularly when evaluating logistic regression models with binary outcomes, the AUC is frequently referred to as the \emph{C-statistic} or \emph{concordance statistic} \citep{Austin_Steyerberg_2012}. This term was introduced by \citet{Harrell_Califf_Pryor_Lee_Rosati_1982} as representing the probability that a randomly selected positive instance receives a higher predicted score than a randomly selected negative instance.

This probabilistic interpretation serves as the bridge to non-parametric statistics. The U statistic in the Mann--Whitney test represents the number of correctly ordered pairs in all pairwise comparisons between the two groups, where one observation comes from each group, with ties typically counted as 0.5. It can be shown that the area under the ROC curve is equivalent to the Mann-Whitney U statistic divided by the product of the sample sizes of the two groups:

\[\text{AUC} = \frac{U}{n_1  n_0}\]

This equality allows confidence intervals for AUC to be derived from the distributional properties of the Mann--Whitney U statistic.

In the context of stochastic ordering tests, such as Mann-Whitney or Brunner--Munzel, the AUC can serve as a measure of effect size and be interpreted as the \emph{probability of superiority}, as it represents the the likelihood that a randomly chosen positive case ranks higher than a randomly chosen negative case \citep{Hanley_McNeil_1982, Bamber_1975}. It is also be referred to as a \emph{relative treatment effect} or \emph{stochastic superiority statistic} in this context \citep{Brunner_Munzel_2000, Karch_2021}.

In the behavioral sciences and psychology, similar concepts have been developed to quantify effect sizes. The \emph{Common Language Effect Size} (CL/CLES) \citep{McGraw_Wong_1992} defines an effect size as, again, the probability that a score sampled at random from one distribution exceeds a score sampled from another. The name reflects its purpose: to provide an intuitively interpretable metric for audiences without formal statistical training. While the original formulation by McGraw and Wong assumed normal distributions, the measure was later generalized and, under the name \emph{A measure of stochastic superiority}, or \emph{Vargha--Delaney A} \citep{Vargha_Delaney_2000}, became mathematically equivalent to the AUC.

While the AUC ranges from 0.5 (random classification) to 1.0 (perfect classification), many disciplines prefer a measure scaled symmetrically around zero to express the strength and direction of association or discriminatory power. This motivates the use of the Gini coefficient (or Gini index). In classification settings, the Gini coefficient is a simple linear transformation of the AUC, defined as \citep{Hand_Till_2001}:

\[\text{Gini} = 2 \times \text{AUC} - 1\]

The term ``Gini coefficient'' is used predominantly in the credit scoring literature \citep{Anderson_2007, Rezac_Rezac_2011}. To distinguish it from the classical Gini index in economics, which measures income or wealth inequality, the name \emph{pseudo-Gini} is sometimes employed \citep{Idczak_2019}. In other application domains, mathematically equivalent quantities may appear under different names.

By construction, the Gini coefficient takes values in (-1, 1) with zero indicating no discriminatory power and negative values corresponding to systematically reversed predictions. Owing to this symmetry, it admits an interpretation analogous to that of a correlation coefficient. Indeed, some authors, such as \citet{Wendt_1972}, have proposed the rank-biserial correlation in a form that is mathematically equivalent to the Gini coefficient, although this term is occasionally used for closely related but non-identical formulations \citep{Rubia_2022}.

An alternative derivation, independent of ROC analysis, arises from the Cumulative Accuracy Profile (CAP) curve \citep{Sobehart_Keenan_Stein_2000}. The CAP plots the cumulative proportion of positive cases against the cumulative proportion of the population, sorted by predicted scores. From this curve, the \emph{Accuracy Ratio} (\emph{AR}) is defined as the ratio of the area between the model CAP and the random CAP to the area between the ideal CAP and the random CAP. It can be shown that

\[\text{AUC} = \frac{\text{AR} + 1}{2}\]

\citep{Engelmann_Hayden_Tasche_2003}, demonstrating that although AR originates from the CAP curve rather than the ROC curve, it conveys exactly the same information as the Gini coefficient.

Just like AUC, the Gini coefficient admits a probabilistic interpretation. If we randomly sample one positive and one negative case and compare their scores, the Gini coefficient equals the difference between the probability that the positive case receives a higher score than the negative case and the probability of the reverse. \citet{Agresti_2010} denotes AUC by \(\alpha\) and the Gini coefficient by \(\Delta\), referring to both as measures of \emph{stochastic superiority}. \citet{Cliff_1993} describes the effect size measure \(d\) (or \(\delta\) for the population), which adopts a similar probabilistic perspective on rank-based comparisons. This measure---identical to the Gini coefficient---is later sometimes referred to as \emph{Cliff's delta} \citep{Bais_Van_Der_Neut_2022}.

Somers' D is a measure of association between a predictor X and an ordinal dependent variable, based on the comparison of concordant and discordant pairs \citep{Somers_1962}. When Y is binary, Somers' D reduces to 2AUC-1 \citep{Newson_2002}. Thus, in binary classification settings, the Gini coefficient can be interpreted as Somers' D with a binary response variable.

It is worth mentioning that ROC curves (under the name of \emph{relative} operating characteristics curves) are also used in meteorology to assess probabilistic forecasts for binary events. In this context, the AUC may be referred to as the ROC score, while the Gini coefficient is known as the ROC skill score \citep{Kharin_Zwiers_2003, Mason_Graham_1999}.

\section{Motivation for computationally efficient AUC calculation}\label{motivation-for-computationally-efficient-auc-calculation}

A single calculation of the AUC is often insufficient for a comprehensive assessment of model quality. More advanced inference, validation, optimization, feature evaluation or simulation procedures require repeated computation of the AUC, placing substantial demands on the computational efficiency of the underlying algorithms.

Resampling methods are commonly used for inference on the area under the ROC curve. Confidence intervals for the AUC can be obtained using bootstrapping or cross-validation \citep{Noma_Matsushima_Ishii_2021, Gu_Ghosal_Roy_2008, LeDell_Petersen_Van_Der_Laan_2015}. Permutation tests enable the assessment of statistical significance, either under the null hypothesis of no discriminatory power or when comparing the performance of two models \citep{Pauly_Asendorf_Konietschke_2016, Venkatraman_2000, Bandos_Rockette_Gur_2006}. Resampling procedures are essential for evaluating uncertainty, model stability, and statistical significance. However, despite their favorable statistical properties, including robustness \citep{Simonoff_Hochberg_Reiser_1986, Pauly_Asendorf_Konietschke_2016}, these methods rely on repeated recalculation of the AUC and can therefore impose considerable computational burdens, particularly for large datasets or complex models \citep{LeDell_Petersen_Van_Der_Laan_2015}.

AUC maximization is another example of computational challenge that has recently gained importance in the development of ranking and classification algorithms. Directly optimizing AUC, rather than surrogate losses, leads to improved generalization and discrimination capabilities in large-scale settings \citep{Yuan_Yan_Sonka_Yang_2021, Zhang_Xu_2022, LeDell_etal_2016}. Unlike many other objectives, the AUC function is non-decomposable over individual examples, which complicates the use of standard optimization techniques \citep{Zhou_Ying_Skiena_2020}. A comprehensive overview of the last 20+ years of AUC maximization research, emphasizing its relevance for imbalanced learning, large datasets, and deep learning applications has been provided by \citet{Yang_Ying_2023}. Many, if not most, approaches to AUC maximization require repeated multiple computations of the AUC statistic \citep[see, for example][]{Calders_Jaroszewicz_2007, Yang_Zhou_Lei_Ying_2020, Ataman_Street_Zhang_2006, Gajowniczek_Zabkowski_2021}.

Efficient AUC computation is also a prerequisite for many feature selection and assessment techniques. Permutation-based variable importance measures were, for example, proposed by \citet{Janitza_Strobl_Boulesteix_2013} or \citet{Biecek_2018}, while AUC-based feature selection algorithms were, inter alia, presented by \citet{Sun_Wang_Wei_2017}; \citet{Su_Du_Wang_Wei_Liu_2022}, \citet{Xu_Suzuki_2014} or \citet{Ribeiro_2021}. Other applications of repeated AUC computation arise in simulation studies. For instance, \citet{Kochanski_2021} introduced a simulation model for risk and pricing competition in which each bank optimizes its credit scoring strategy, requiring numerous AUC evaluations to assess performance under varying market conditions.

In summary, efficient and scalable computation of AUC has become a foundational requirement not only for model evaluation but also for advanced inference, optimization, feature selection, and simulation studies. As AUC-based methods continue to expand into areas such as deep models and large-scale applications, the computational complexity of repeated AUC calculations remains a critical challenge.

\section{AUC computation implementation in R packages}\label{auc-computation-implementation-in-r-packages}

Table 1 presents a selection of R packages and functions commonly used to calculate the area under the ROC curve (AUC). For each package, the table lists the primary function, typical usage syntax, the computational method employed, and the programming language(s) in which the core implementation is written. The computational methods range from rank-based approaches and pairwise comparisons to trapezoidal integration, reflecting different trade-offs between accuracy and efficiency. Some functions rely entirely on R, while others leverage compiled code in C++ for improved performance. It is important to note that certain functions perform additional checks or compute supplementary metrics beyond the AUC itself, which may reduce computational efficiency when only the AUC is needed. The practical impact of these differences will be examined empirically in the following section.

\begin{table}[H]
\centering
\caption{\label{tab:unnamed-chunk-1}Functions computing AUC.}
\centering
\resizebox{\ifdim\width>\linewidth\linewidth\else\width\fi}{!}{
\begin{tabular}[t]{ll>{}lll}
\toprule
Package & Function & Usage & Method & Language\\
\midrule
bigstatsr & AUC & \ttfamily{AUC(-pred, target)} & Pairwise comparison (optimized) & C++, R\\
\addlinespace
caTools & colAUC & \ttfamily{colAUC(-pred, target)[1, 1]} & Rank sum & R\\
\addlinespace
cvAUC & AUC & \ttfamily{AUC(-pred, target)} & Trapezoidal rule & R\\
\addlinespace
DescTools & Cstat & \ttfamily{Cstat(-pred, target)} & Pairwise comparison (optimized) & C++, R\\
\addlinespace
effsize & VD.A & \ttfamily{VD.A(pred \textasciitilde{} factor(target))\$estimate} & Rank sum & R\\
\addlinespace
fbroc & boot.roc & \ttfamily{boot.roc(-pred, as.logical(target))\$auc} & Trapezoidal rule & C++, R\\
\addlinespace
Hmisc & somers2 & \ttfamily{somers2(-pred, target)["C"]} & Rank sum & R\\
\addlinespace
MLmetrics & AUC & \ttfamily{AUC(-pred, target)} & Rank sum & R\\
\addlinespace
mltools & auc\_roc & \ttfamily{auc\_roc(-pred, target)} & Trapezoidal rule & R\\
\addlinespace
ModelMetrics & auc & \ttfamily{auc(target, -pred)} & Rank sum & C++, R\\
\addlinespace
precrec & evalmod & \ttfamily{evalmod(mmdata(scores = -pred, labels = target), mode="aucroc")\$uaucs\$aucs} & Rank sum & C++, R\\
\addlinespace
pROC & auc & \ttfamily{auc(target, pred, lev=c('0', '1'), dir=">")} & Trapezoidal rule & R\\
\addlinespace
rcompanion & vda & \ttfamily{vda(x = pred[target == 0], y = pred[target == 1], digits=100)} & Rank sum & R\\
\addlinespace
ROCR & performance & \ttfamily{performance(prediction(-pred, target), "auc")@y.values[[1]]} & Trapezoidal rule & R\\
\addlinespace
scikit-learn & roc\_auc\_score & \ttfamily{import("sklearn.metrics")\$roc\_auc\_score(target, -pred)} & Trapezoidal rule & Python, C\\
\addlinespace
scorecard & perf\_eva & \ttfamily{perf\_eva(-pred, target, binomial\_metric = "auc", show\_plot=FALSE)\$binomial\_metric\$dat\$AUC} & Trapezoidal rule & R\\
\addlinespace
yardstick & roc\_auc\_vec & \ttfamily{roc\_auc\_vec(as.factor(target), pred)} & Trapezoidal rule & R\\
\addlinespace
DescTools & SomersDelta & \ttfamily{SomersDelta(-pred, target)/2 + 1/2} & Pairwise comparison (unoptimized) & C++, R\\
\bottomrule
\end{tabular}}
\end{table}

Based on the reviewed functions, we identified three main types of algorithms for AUC computation. These approaches differ in how they process predicted scores and true labels, we identified three main types of algorithms for AUC computation: (1) trapezoidal integration over the ROC curve, (2) (optimized) pairwise
comparison, (3) and rank-based (Mann--Whitney U statistic formulation).

Let \((s_i, y_i)\) for \(i = 1, \dots, n\) denote the score \(s\) assigned to an account and its corresponding true label. In line with standard practice in credit scoring, we assume that \(y_i=1\) indicates a ``bad'' account (e.g., one that defaults or fails to repay the loan), while \(y_i=0\) represents a good account. A properly functioning scoring should assign lower scores to accounts with a higher predicted probability of being
bad, and higher scores to those likely to be good.

Let \(n_1 = \sum_{i=1}^{n}\mathbb{I}(y_i=1)\) denote the number of bad accounts, and \(n_0 =\sum_{i=1}^{n}\mathbb{I}(y_i=0)\) -- number of good accounts.

The three computational approaches described above can be expressed formally as follows:

\textbf{1. Trapezoidal integration over the ROC curve}

This approach computes the AUC as the area under the empirical ROC curve by applying the trapezoidal rule:

\[\text{AUC} = \sum_{k=1}^{m-1}(\text{FPR}_{k+1} - \text{FPR}_{k})\cdot\frac{\text{TPR}_{k+1}+\text{TPR}_k}{2}\]

where \(m\) is the number of distinct score thresholds from lowest to the highest, \(TPR_k=TP_k/n_1\) is the true positive rate, \(FPR_k=FP_k/n_0\) is the false positive Rate at threshold \(k\), \(TP_k\) is the number of true positives, \(\text{FP}_k\) is the number of false positives, \(n_1\) is the number of positive cases, \(n_0\) is the number of negative cases.

\textbf{2. Optimized pairwise comparison}

In this formulation, the AUC is based on the probabilitic (pairwise comparison) definition, that is the probability that a randomly chosen positive instance receives a higher score than a randomly chosen negative instance, with 0.5 applied to ties.

\[\text{AUC} = \frac{1}{n_1 n_0}\sum_{i:y_i=1}\sum_{j:y_j=0}\left[\mathbb{I}(s_i<s_j)+\frac{1}{2}\mathbb{I}(s_i=s_j)\right]\]

\textbf{3. Rank-based formula}

The rank-based formula for AUC is directly based on the method developed for the Mann--Whitney U statistic computation \citep[see][]{Sheskin_2020}:

\[\text{AUC} = \frac{\bar{R}_{1}-{n_1(n_1+1)}/2}{n_0}\]

where \(\bar{R}_{1}\) is the mean rank for \(s_i\) where \(y_i=1\):

\[\bar{R}_{1} = \frac{1}{n_1}\sum_{i:y_i=1}\text{Rank}(s_i)\]

\section{Efficiency study}\label{efficiency-study}

\subsection{Performance of existing R Packages}\label{performance-of-existing-r-packages}

The computational efficiency of several R packages for AUC computation was evaluated through a series of benchmarks. In each benchmark, every AUC function was invoked 100 times. Experiments were conducted on three simulated datasets of increasing size, containing 1,000, 10,000, and 100,000 observations. The predictor variable \emph{pred} was synthetically generated such that the first half of the observations followed a \(\mathcal{N}(0,1)\) distribution, while the remaining observations followed a \(\mathcal{N}(1,1)\) distribution. The corresponding response variable was binary, with the first half of the observations assigned label 1 and the second half assigned label 0.

\textbf{First benchmark - 1000 observations}

\begin{center}\includegraphics[width=1\linewidth]{report_files/figure-latex/package_benchmark_1-1} \end{center}

\textbf{Second benchmark - 10000 observations}

\begin{center}\includegraphics[width=1\linewidth]{report_files/figure-latex/package_benchmark_2-1} \end{center}

\textbf{Third benchmark - 100000 observations}

\begin{center}\includegraphics[width=1\linewidth]{report_files/figure-latex/package_benchmark_3-1} \end{center}

The computational time required to evaluate the AUC metric varies substantially across the examined implementations, spanning several orders of magnitude. For instance, the \emph{bigstatsr} implementation computes the AUC nearly two orders of magnitude faster than the corresponding function in the \emph{scorecard} package. This discrepancy may be attributable to several factors, including additional computations performed internally by some functions (e.g., estimation of confidence intervals or simultaneous evaluation of multiple performance metrics), differences in algorithmic design, or a combination of these effects.

Overall, \emph{bigstatsr} consistently provides the fastest AUC computation. Although \emph{MLMetrics} and \emph{Hmisc} exhibit slightly lower execution times on the smallest dataset, \emph{bigstatsr} clearly outperforms these alternatives as the number of observations increases.

Consequently, when an existing R implementation is preferred, we recommend the use of the \emph{AUC} function from the \emph{bigstatsr} package for efficient computation of the AUC metric, particularly in large-scale settings.

\subsection{Benchmarking custom R implementations}\label{benchmarking-custom-r-implementations}

The three identified algorithms were evaluated through direct comparison using custom R implementations designed to be as computationally efficient as possible. Each implementation was restricted to the core AUC computation and deliberately omitted input validation and auxiliary checks in order to minimize overhead and isolate algorithmic performance. All benchmarks were conducted on the same datasets used in the package-level experiments.

\textbf{First benchmark - 1000 observations}

\begin{center}\includegraphics[width=1\linewidth]{report_files/figure-latex/own_function_benchmark_1-1} \end{center}

\textbf{Second benchmark - 10000 observations}

\begin{center}\includegraphics[width=1\linewidth]{report_files/figure-latex/own_function_benchmark_2-1} \end{center}

\textbf{Third benchmark - 100000 observations}

\begin{center}\includegraphics[width=1\linewidth]{report_files/figure-latex/own_function_benchmark_3-1} \end{center}

Across all benchmark settings, the optimized pairwise comparison method consistently exhibited the shortest execution time, with the trapezoidal method performing only marginally slower. In contrast, the rank-sum method showed a pronounced increase in computational cost as the number of observations grew.

These results suggest that, when developing new R functions for AUC computation, the pairwise comparison approach provides the most efficient algorithmic foundation, particularly in large-scale scenarios.

\bibliography{RJreferences.bib}

\address{%
Błażej Kochański\\
\\%
\\
%
%
%
%
}

\address{%
Przemysław Peplinski\\
Gdańsk University of Technology\\%
Faculty of Management and Economics\\ Gdańsk\\
%
%
%
%
}

\address{%
Miriam Nieslona\\
\\%
\\
%
%
%
%
}

\address{%
Wiktor Galewski\\
\\%
\\
%
%
%
%
}

\address{%
Piotr Geremek\\
\\%
\\
%
%
%
%
}
