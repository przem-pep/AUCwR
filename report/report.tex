% !TeX root = RJwrapper.tex
\title{Area under ROC curve -- review and efficient computation in R}


\author{by Błażej Kochański, Przemysław Pepliński, Miriam Nieslona, Wiktor Galewski, and Piotr Geremek}

\maketitle

\abstract{%
The AUC (Area Under the Curve) measure is widely used in statistical classification and machine learning, including credit scoring, where it is employed to assess the quality of predictive models. The goal of this paper is to review methods for calculating the AUC measure, followed by an analysis of the efficiency of computing this measure in R.
}

\section{Introduction}\label{introduction}

ROC curves\ldots{}

\section{Background}\label{background}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{report_files/figure-latex/auc-gini-1} 

}

\caption{Geometric interpretation: AUC (AUROC) is the area under the ROC curve, Gini is twice the area between the diagonal y=x and the ROC curve.}\label{fig:auc-gini}
\end{figure}

\section{AUC -- alternative names and formulas}\label{auc-alternative-names-and-formulas}

AUC is referred to as:

\begin{itemize}
\item
  C-statistic {[}Harrell et al, 1982{]}
\item
  a version of estimator of the common language effect size statistic
\item
  probability of superiority {[}Hanley \& McNeil, 1982{]}
\item
  ``Vargha \& Delaney A'' (``measure of stochastic superiority'')
  \citep{Vargha_Delaney_2000}
\item
  ``relative treatment effect'' / ``stochastic superiority statistic'' in
  the Brunner-Munzel test
\end{itemize}

Gini coefficient is also referred to as:

\begin{itemize}
\item
  pseudo Gini \citep{Idczak_2019}
\item
  a version of the rank-biserial correlation coefficient
\item
  accuracy ratio based on the Cumulative Accuracy Profile {[}Engelmann
  et al., 2003{]}
\item
  a special case of the Somers' D statistic {[}Newson, 2002{]}
\item
  Cliff's delta \citep{Cliff_1993}
\end{itemize}

\textbf{Relation to the U statistic in the Mann-Whitney.}

There is a direct relationship between AUC and the Mann-Whitney U
statistic.

AUC is equivalent to the probability that a randomly selected positive
case will have a higher score than a randomly selected negative case
{[}Hanley \& McNeil, 1982{]};\citep{Bamber_1975}.
\[\text{AUC} = \frac{U}{n_1 \times n_0}\] where \(U\) is the number of
pairs in which the ``bad'' score is \textless{} the ``good'' score, \(n_1\) is the
number of `bad' scores, and \(n_0\) is the number of ``good'' scores.

This relationship provides the statistical basis for treating AUC as a
measure of discrimination and allows confidence intervals to be
constructed using Mann-Whitney statistics theory {[}Hanley \& McNeil,
1982{]}.

\textbf{Relation to the Somers' D.}

Somers' D coefficient is a measure of association between ordinal
variables, which also shows a direct relationship with AUC. It can be
considered a generalization of AUC for ordinal variables {[}Newson, 2002{]}.

Somers' D is defined based on concordant and discordant pairs. A pair of
observations (i,j) is concordant if the ranking of the independent
variable X and the ranking of the dependent variable Y are in the same
order, i.e., if (X\_i - X\_j) and (Y\_i - Y\_j) have the same sign. A pair
is discordant if the signs are opposite. Somers' D is calculated as the
difference between the number of concordant and discordant pairs,
divided by the number of unrelated pairs on the independent variable X
{[}Somers, 1962{]}.

In binary classification, Somers' D is equal to 2×AUC-1, which links it
directly to AUC {[}Newson, 2002{]}.
\[\text{Somers' D} = 2 \cdot \text{AUC} - 1\]

\textbf{Cumulative Accuracy profile.}

The CAP curve is a graphical tool used to evaluate the performance of
classification models, primarily in the area of creditworthiness
assessment. It shows the cumulative percentage of positive cases
relative to the cumulative percentage of the entire population, sorted
by predicted probability of default. {[}Engelmann et al., 2003{]}.

Accuracy Ratio (AR): \[\text{AR} = \frac{A}{B}\] where \(A\) is the area
between the model CAP and the random CAP, and \(B\) is the area between
the ideal CAP and the random CAP. Areas A and B are calculated using the
trapezoidal method {[}Engelmann et al., 2003{]}.

The relationship between AUC and the CAP curve can be described by the
following formula {[}Engelmann et al., 2003{]}:
\[\text{AUC} = \frac{\text{AR} + 1}{2}\]

\section{The need for computational efficiency in AUC estimation}\label{the-need-for-computational-efficiency-in-auc-estimation}

Opisać:

\begin{itemize}
\item
  AUC / Gini bootstrapping / permutation tests

  \begin{itemize}
  \tightlist
  \item
    including DALEX (?)
  \end{itemize}
\item
  AUC optimization algorithms
\item
  credit market simulation
\end{itemize}

\citet{Kochanski_2021} proposed simulation\ldots{}

Simulation \citep{Kochanski_2021}\ldots{}

\begin{itemize}
\tightlist
\item
  inne \ldots{}
\end{itemize}

\section{R packages}\label{r-packages}

--\textgreater{} Tutaj należy wstawić tabelkę pana Wiktora \textless--

We identified three main types of algorithms for AUC computation: (1)
trapezoidal integration over the ROC Curve, (2) (optimized) pairwise
comparison, (3) and rank-based (Mann--Whitney U statistic formulation).

Let \((s_i, y_i)\) for \(i = 1, \dots, n\) denote the score assigned to an
account and its corresponding true label. In line with standard practice
in credit scoring, we assume that \(y_i=1\) indicates a ``bad'' account
(e.g., one that defaults, doesn't repay the loan), while \(y_i=0\)
represents a good account. A properly functioning scoring should assign
lower scores to accounts with a higher predicted probability of being
bad, and higher scores to those likely to be good.

Let \(n_1 = \sum_{i=1}^{n}\mathbb{I}(y_i=1)\) denote the number of bad
accounts, and \(n_0 = \sum_{i=1}^{n}\mathbb{I}(y_i=0)\) -- number of good
accounts.

\textbf{Trapezoidal Integration over the ROC Curve}

\[\text{AUC} = \sum_{k=1}^{m-1}(\text{FPR}_{k+1} - \text{FPR}_{k})\cdot\frac{\text{TPR}_{k+1}+\text{TPR}_k}{2}\]

where \(m\) is the number of distinct score thresholds from lowest to the
highest score, \(TPR_k\) is the True Positive Rate, \(FPR_k\) is the False
Positive Rate. \(TPR_k=TP_k/n_1\) and \(FPR_k=FP_k/n_0\), where \(TP_k\) is
the number of true positives, \(\text{FP}_k\) is the number of false
positives, \(n_1\) is the number of positive cases, \(n_0\) is the number of
negative cases.

\textbf{Optimized Pairwise Comparison}

AUC as the probability that a randomly chosen positive instance receives
a higher score than a randomly chosen negative instance:

\[\text{AUC} = \frac{1}{n_1 n_0}\sum_{i:y_i=1}\sum_{j:y_j=0}\left[\mathbb{I}(s_i<s_j)+\frac{1}{2}\mathbb{I}(s_i=s_j)\right]\]

Generally, naive pairwise comparison is not efficient (?), but \ldots{}

\subsection{Rank-Based (Mann--Whitney U Statistic Formulation)}\label{rank-based-mannwhitney-u-statistic-formulation}

\[\text{AUC} = \frac{\bar{R}_{1}-{n_1(n_1+1)}/2}{n_0}\]

where \(\bar{R}_{1}\) is the mean rank for \(s_i\) where \(y_i=1\):

\[\bar{R}_{1} = \frac{1}{n_1}\sum_{i:y_i=1}\text{Rank}(s_i)\]

\section{Efficiency study}\label{efficiency-study}

\section{Case studies}\label{case-studies}

--\textgreater{} studieS, jeżeli będzie więcej \textless--

\subsection{DALEX package}\label{dalex-package}

--\textgreater{} Wykres generuje się długo, a możemy go przyśpieszyć \textless--

\section{Summary}\label{summary}

\bibliography{RJreferences.bib}

\address{%
Błażej Kochański\\
\\%
\\
%
%
%
%
}

\address{%
Przemysław Pepliński\\
Gdańsk University of Technology\\%
Faculty of Management and Economics\\ Gdańsk\\
%
%
%
%
}

\address{%
Miriam Nieslona\\
\\%
\\
%
%
%
%
}

\address{%
Wiktor Galewski\\
\\%
\\
%
%
%
%
}

\address{%
Piotr Geremek\\
\\%
\\
%
%
%
%
}
