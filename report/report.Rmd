---
title: "Area under ROC curve – review and efficient computation in R"
date: "2025-10-28"
abstract: >
  The AUC (Area Under the Curve) measure is widely used in statistical classification and machine learning, including credit scoring, where it is employed to assess the quality of predictive models. The goal of this paper is to review methods for calculating the AUC measure, followed by an analysis of the efficiency of computing this measure in R.
draft: true
author:  
  - name: Błażej Kochański
  - name: Przemysław Peplinski
    affiliation: Gdańsk University of Technology
    address:
    - Faculty of Management and Economics
    - Gdańsk
  - name: Miriam Nieslona
  - name: Wiktor Galewski
  - name: Piotr Geremek
type: package
output: 
  rjtools::rjournal_article:
    self_contained: yes
    toc: no
bibliography: RJreferences.bib
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(plotly)
library(ggplot2)
library(kableExtra)
library(readr)
library(Rcpp)
library(microbenchmark)
library(reticulate)

sourceCpp("../scripts/AUC_cpp.cpp")
source("../R/AUC_functions.R")
#use_python(Sys.which("python"), required = TRUE)
#roc_auc_score <- import("sklearn.metrics")$roc_auc_score
```

# Introduction

ROC curves...

# Background

```{r auc-gini, fig.align = "center", out.width = "100%", fig.cap="Geometric interpretation: AUC (AUROC) is the area under the ROC curve, Gini is twice the area between the diagonal y=x and the ROC curve.", dev=if(knitr::is_latex_output()) "cairo_pdf" else "png"}
cz = c(.6, .8, .95)
sw = c(.9, .8, .6)

df <- data.frame(FPR = c(0, 1-sw, 1), TPR = c(0, cz, 1))

library(ggplot2)
library(patchwork)

ggplot(df, aes(x = FPR, y = TPR)) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0, ymax = 1, fill = "white", color = "black") +
  geom_area(fill="lightblue") +
  geom_line() +
  geom_point() +
  annotate("text", x = 0.5, y = 0.5, label = "AUC", size =10,  color = "darkblue") +
  coord_fixed(ratio = 1, xlim = c(0,1), ylim = c(0,1)) +
  labs(x = "FPR (1 - specificity)\ncumulative fraction of goods", 
       y = "TPR (sensitivity)\ncumulative fraction of bads") +
  theme_minimal() +

#library(ggplot2)
ggplot(df, aes(x = FPR, y = TPR)) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0, ymax = 1, fill = "white", color = "black") +
  geom_polygon(fill="orange2") +
  geom_polygon(aes(x=TPR, y=FPR), fill="orange1") +
  geom_line() +
  geom_point() +
  annotate("text", x = 0.5, y = 0.5, label = "Gini", size =10,  color = "orange4") +
  coord_fixed(ratio = 1, xlim = c(0,1), ylim = c(0,1)) +
  labs(x = "FPR (1 - specificity)\ncumulative fraction of goods", y = "") +
  theme_minimal()
```

# AUC – alternative names and related measures

The Area Under the Receiver Operating Characteristic (ROC) curve,
commonly abbreviated as AUC, is a fundamental metric for evaluating
the performance of classification models. However, due to its independent
adoption and development across diverse fields, ranging from biomedical
statistics and psychology to econometrics and credit scoring, this measure
appears in the literature under numerous aliases and mathematical formulations. 
Recognizing these equivalences is essential for researchers to leverage 
efficient computational algorithms developed in different domains.

**Alternative names and interpretations of AUC**

In medical statistics and epidemiology, particularly when evaluating logistic
regression models with binary outcomes, the AUC is frequently referred to 
as the $c-statistic$ (or concordance statistic). This term defined by 
Harrell et al. [@Harrell_et_al_1982] and further elaborated by, for example 
Austin and Steyerberg [@Austin_Steyerberg_2012], represents the probability 
of concordance between predicted probabilities and observed outcomes. 
It quantifies the predictive accuracy of a model by estimating the probability 
that a randomly selected subject who experienced the outcome has a higher 
predicted probability than a randomly selected subject who did not.

This probabilistic interpretation serves as the bridge to non-parametric 
statistics. Area under the ROC curve is statistically equivalent to the 
Mann-Whitney U statistic divided by the product of the sample sizes 
of the two groups, where in this context, the metric is often interpreted as
the $probability of superiority$ indicating the likelihood that 
a randomly chosen positive case ranks higher than a randomly chosen 
negative case [@Hanley_McNeil_1982].

In the behavioral sciences and psychology, similar concepts have been developed 
to measure effect sizes. There the AUC corresponds to the 
$Common Language Effect Size$ (CL/CLES) [@McGraw_Wong_1992], which 
converts an effect size into a probability, specifically defined as the 
probability that a score sampled at random from one distribution will be 
greater than a score sampled from another. This metric was developed to provide 
an index of effect size that is intuitively interpretable by audiences without 
statistical training. While the original formulation by McGraw and Wong focused 
on normal distributions, the later proposed $measure of stochastic superiority$
(denoted as $A$) [@Vargha_Delaney_2000] generalized CLES 
concept so that it could be applicable to any variable that is at least 
ordinally scaled, regardless of the distribution form, and is identical to the
CL statistic in the continuous case. Furthermore, in the context of 
non-parametric analysis for longitudinal data the ***relative treatment effect
was defined [@Brunner_et_al_2002]. This metric estimates the probability that 
an observation from one group is smaller than an observation from another, 
effectively serving as a probabilistic analog to the AUC 
[Brunner_et_al_2002],[Engelmann_et_al_2003].


**The Gini Coefficient and other AUC related measures**

While the AUC scales from 0.5 (random classification) to 1.0 
(perfect classification), many disciplines prefer a measure scaled between 
0 and 1 (or -1 and 1) to represent the strength of association or 
discriminatory power. This leads to the Gini coefficient (or Gini index). 
The Gini coefficient used in classification contexts is simply a linear 
transformation of the AUC, defined by the relationship [@Hand_Till_2001]:
$$\text{Gini} = 2 \cdot \text{AUC} - 1$$
Cliff [1993] discusses an ordinal statistic, $d$, which compares the number of 
times a score from one group is higher than one from another versus the reverse.
This statistic of ordinal dominance, later named $Cliff’s Delta$ 
[@Bais_van_der_Neut_2022] was described as a non-parametric effect size based 
on data observations, which quantifies the difference between the probability 
that a value from one group is higher than a value from the other and 
the reverse probability. 

**Relation to the U statistic in the Mann-Whitney.**

There is a direct relationship between AUC and the Mann-Whitney U
statistic.

AUC is equivalent to the probability that a randomly selected positive
case will have a higher score than a randomly selected negative case
[@Hanley_McNeil_1982],[@Bamber_1975].
$$\text{AUC} = \frac{U}{n_1 \times n_0}$$ where $U$ is the number of
pairs in which the “bad” score is \< the “good” score, $n_1$ is the
number of ‘bad’ scores, and $n_0$ is the number of “good” scores.

This relationship provides the statistical basis for treating AUC as a
measure of discrimination and allows confidence intervals to be
constructed using Mann-Whitney statistics theory [@Hanley_McNeil_1982].

**Relation to the Somers' D.**

Somers' D coefficient is a measure of association between ordinal
variables, which also shows a direct relationship with AUC. It can be
considered a generalization of AUC for ordinal variables [@Newson_2002].

Somers' D is defined based on concordant and discordant pairs. A pair of
observations (i,j) is concordant if the ranking of the independent
variable X and the ranking of the dependent variable Y are in the same
order, i.e., if (X_i - X_j) and (Y_i - Y_j) have the same sign. A pair
is discordant if the signs are opposite. Somers' D is calculated as the
difference between the number of concordant and discordant pairs,
divided by the number of unrelated pairs on the independent variable X
[@Somers_1962].

In binary classification, Somers' D is equal to 2×AUC-1, which links it
directly to AUC [@Newson_2002].
$$\text{Somers' D} = 2 \cdot \text{AUC} - 1$$

**Cumulative Accuracy profile.**

The CAP curve is a graphical tool used to evaluate the performance of
classification models, primarily in the area of creditworthiness
assessment. It shows the cumulative percentage of positive cases
relative to the cumulative percentage of the entire population, sorted
by predicted probability of default. [@Engelmann_Hayden_Tasche_2003].

Accuracy Ratio (AR): $$\text{AR} = \frac{A}{B}$$ where $A$ is the area
between the model CAP and the random CAP, and $B$ is the area between
the ideal CAP and the random CAP. Areas A and B are calculated using the
trapezoidal method [@Engelmann_Hayden_Tasche_2003].

Within the credit scoring literature, the AR metric is sometimes referred to 
as the $pseudo Gini$ index to differentiate it from the classical Gini index 
used in economics to measure inequality. It was established that the 
pseudo Gini index is based on the concentration curve (CAP) and that its value 
is always equal to the Accuracy Ratio (referred to as Accuracy Rate) 
for any scoring model[@Idczak_2019].

The relationship between AUC and the CAP curve can be described by the
following formula [@Engelmann_Hayden_Tasche_2003]:
$$\text{AUC} = \frac{\text{AR} + 1}{2}$$

# The need for computational efficiency in AUC estimation

Opisać:

-   AUC / Gini bootstrapping / permutation tests

    -   including DALEX (?)

-   AUC optimization algorithms

-   credit market simulation

@Kochanski_2021 proposed simulation... <!-- Kochanski (2021) -->

Simulation [@Kochanski_2021]... <!-- Simulation (Kochanski 2021) --> 

-   inne ...

# R packages

```{r}
functions_table <- read.csv("../functions_table.csv", sep = ";")

kable(functions_table, caption = "Table 1: Functions computing AUC.") %>%
  kable_styling(font_size = 7)
```

We identified three main types of algorithms for AUC computation: (1)
trapezoidal integration over the ROC Curve, (2) (optimized) pairwise
comparison, (3) and rank-based (Mann–Whitney U statistic formulation).

Let $(s_i, y_i)$ for $i = 1, \dots, n$ denote the score assigned to an
account and its corresponding true label. In line with standard practice
in credit scoring, we assume that $y_i=1$ indicates a "bad" account
(e.g., one that defaults, doesn't repay the loan), while $y_i=0$
represents a good account. A properly functioning scoring should assign
lower scores to accounts with a higher predicted probability of being
bad, and higher scores to those likely to be good.

Let $n_1 = \sum_{i=1}^{n}\mathbb{I}(y_i=1)$ denote the number of bad
accounts, and $n_0 = \sum_{i=1}^{n}\mathbb{I}(y_i=0)$ – number of good
accounts.

**Trapezoidal Integration over the ROC Curve**

$$\text{AUC} = \sum_{k=1}^{m-1}(\text{FPR}_{k+1} - \text{FPR}_{k})\cdot\frac{\text{TPR}_{k+1}+\text{TPR}_k}{2}$$

where $m$ is the number of distinct score thresholds from lowest to the
highest score, $TPR_k$ is the True Positive Rate, $FPR_k$ is the False
Positive Rate. $TPR_k=TP_k/n_1$ and $FPR_k=FP_k/n_0$, where $TP_k$ is
the number of true positives, $\text{FP}_k$ is the number of false
positives, $n_1$ is the number of positive cases, $n_0$ is the number of
negative cases.

**Optimized Pairwise Comparison**

AUC as the probability that a randomly chosen positive instance receives
a higher score than a randomly chosen negative instance:

$$\text{AUC} = \frac{1}{n_1 n_0}\sum_{i:y_i=1}\sum_{j:y_j=0}\left[\mathbb{I}(s_i<s_j)+\frac{1}{2}\mathbb{I}(s_i=s_j)\right]$$

Generally, naive pairwise comparison is not efficient (?), but ...

## Rank-Based (Mann–Whitney U Statistic Formulation)

$$\text{AUC} = \frac{\bar{R}_{1}-{n_1(n_1+1)}/2}{n_0}$$

where $\bar{R}_{1}$ is the mean rank for $s_i$ where $y_i=1$:

$$\bar{R}_{1} = \frac{1}{n_1}\sum_{i:y_i=1}\text{Rank}(s_i)$$

# Efficiency study

## Benchmarking packages

All of the R packages that can be used to calculate the AUC measure have had their performance speed tested. In every benchmark, each function was called 100 times, and there are a total of three benchmarks, each with a different number of observations, all of which have been generated from the normal distribution.

``` {r}
# First dataset (n = 1000)

pred1 <- c(rnorm(500), rnorm(500, 1))

target1 <- c(rep(1, 500), rep(0, 500))

# Second dataset (n = 10000)

pred2 <- c(rnorm(5000), rnorm(5000, 1))

target2 <- c(rep(1, 5000), rep(0, 5000))

# Third dataset (n = 100000)

pred3 <- c(rnorm(50000), rnorm(50000, 1))

target3 <- c(rep(1, 50000), rep(0, 50000))


call_benchmark <- function(pred, target, times = 100) {
  
  my_benchmark <- microbenchmark(
    
    ROCR::performance(ROCR::prediction(-pred, target), "auc")@y.values[[1]],
    
    pROC::auc(target, pred, lev=c('0', '1'), dir=">"),
    
    mltools::auc_roc(-pred, target),
    
    Hmisc::somers2(-pred, target)["C"],
    
    bigstatsr::AUC(-pred, target),
    
    scorecard::perf_eva(-pred, target, binomial_metric = "auc", show_plot = FALSE)$binomial_metric$dat$AUC,
    
    caTools::colAUC(-pred, target)[1, 1],
    
    precrec::evalmod(precrec::mmdata(scores = -pred, labels = target), mode = "aucroc")$uaucs$aucs,
    
    roc_auc_score(target, -pred),
    
    yardstick::roc_auc_vec(as.factor(target), pred),
    
    (rcompanion::cliffDelta(x = pred[target == 0], y = pred[target == 1], digits = 100) + 1) / 2,
    
    ModelMetrics::auc(target, -pred),
    
    MLmetrics::AUC(-pred, target),
    
    cvAUC::AUC(-pred, target),
    
    fbroc::boot.roc(-pred, as.logical(target))$auc,
    
    DescTools::Cstat(-pred, target),
    
    effsize::VD.A(pred ~ factor(target))$estimate,
    
    rcompanion::vda(x = pred[target == 0], y = pred[target == 1], digits=100),
    
    times = times
  )
  
  levels(my_benchmark$expr) <- c("ROCR", "pROC",
                                 "mltools", "Hmisc", "bigstatsr", "scorecard",
                                 "caTools", "precrec", "scikit-learn",
                                 "yardstick", "rcompanion::cliffDelta", "ModelMetrics", "MLmetrics",
                                 "cvAUC", "fbroc", "DescTools", "effsize", "rcompanion::vda"
                                 )
  
  my_benchmark$expr <- reorder(my_benchmark$expr, -my_benchmark$time, FUN = median)
  
  autoplot(my_benchmark) +
    theme(axis.text.y = element_text(size = rel(1.5)))
}


```

## First benchmark - 1000 observations

``` {r}
#call_benchmark(pred1, target1)

```

## Second benchmark - 10000 observations

``` {r}
#call_benchmark(pred2, target2)

```

## Third benchmark - 100000 observations

``` {r}
#call_benchmark(pred3, target3)

```


# Case studies

--\> studieS, jeżeli będzie więcej \<--

## DALEX package

--\> Wykres generuje się długo, a możemy go przyśpieszyć \<--

# Summary
