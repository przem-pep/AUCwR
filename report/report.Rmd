---
title: "Area under ROC curve – review and efficient computation in R"
date: "2025-12-31"
abstract: >
  The Area Under the Receiver Operating Characteristic Curve (AUC) is a widely used measure for evaluating the performance of binary classification models. In the literature and in practice, it appears under various names and is closely related to other performance measures. In this paper, we review these formulations and discuss the motivation for efficient AUC computation in empirical analysis. We survey R packages that provide implementations of AUC and describe the algorithms they employ. We then conduct a benchmarking study to compare the execution time of selected implementations. **A case study illustrates how the choice of AUC computation method can substantially reduce computation time and accelerate analytical workflows.**
draft: true
author:  
  - name: Błażej Kochański
  - name: Przemysław Peplinski
    affiliation: Gdańsk University of Technology
    address:
    - Faculty of Management and Economics
    - Gdańsk
  - name: Miriam Nieslona
  - name: Wiktor Galewski
  - name: Piotr Geremek
type: package
output: 
  rjtools::rjournal_article:
    self_contained: yes
    toc: no
bibliography: RJreferences.bib
editor_options: 
  markdown: 
    wrap: 72
header-includes:
  - \usepackage{float}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE)

library(plotly)
library(ggplot2)
library(kableExtra)
library(microbenchmark)

```

# Introduction

Assessing the quality of predictive models is a critical stage in
machine learning and statistical inference processes. Among the many
available metrics, the Area Under the Receiver Operating Characteristic
curve (AUC) has established itself as a standard measure of
discrimination, particularly in binary classification problems. The
popularity of this metric stems from its independence from the chosen
cut-off point and its robustness to class imbalance, making it useful in
diverse fields such as medical diagnostics, psychology, and credit risk
assessment.

Modern statistical applications require not only methodological
correctness but also high computational efficiency. In the era of Big
Data and increasing complexity of validation procedures, a single
calculation of the AUC is often insufficient. Similar challenges are
posed by simulation market models in banking, where generating ROC
curves without historical data serves to estimate the impact of scoring
models on portfolio profitability. In such scenarios, classical
algorithm implementations can become a computational bottleneck,
rendering procedures like the bootstrap impractical.

The goal of this paper is to review methods for calculating the AUC
measure and to analyze the efficiency of available implementations
within the R environment. In the first part of the work, we organize
definitions and highlight the mathematical links between AUC and other
measures, such as the Gini coefficient. Next, we discuss the main
classes of algorithms used for AUC estimation: trapezoidal integration,
optimized pairwise comparisons, and rank-based approaches. Finally, we
present the results of performance benchmarks for popular R packages and
propose our alternatives, identifying optimal solutions for different
sizes of large datasets.

# ROC curve and AUC

The ROC curve is one of the most widely used tools for assessing the performance of binary classifiers. Introduced in the 1940s for radar signal analysis, the ROC curve derives its name from “Receiver Operating Characteristic” [@Junge_Dettori_2018]. Today, ROC curves are used across diverse fields—such as medicine, biotechnology, computer science, and banking—to evaluate scoring and classification models.

The ROC curve is plotted within the unit square, connecting points defined by sensitivity and specificity at various decision thresholds ("cut-offs"). In an ROC curve, the x-axis represents the false positive rate (FPR, or “1 - specificity”), the proportion of false positives among all actual negatives. The y-axis represents sensitivity (or “recall”), the proportion of correctly identified positives among all actual positives (true positive rate, TPR) [@Fawcett_2006].

The AUC (Area Under the ROC Curve) provides a single-number summary of the classification performance of binary models across all possible decision thresholds. A ROC curve that passes through the point (0, 1) corresponds to an ideal classifier, achieving zero false positives and perfect detection of positive cases. Curves lying closer to this point are therefore more desirable [@Hanley_McNeil_1982]; they correspond to larger values of the Area Under the Curve (AUC). By contrast, the diagonal line connecting (0, 0) and (1, 1) represents a random classifier, for which the expected value of AUC equals 0.5.

In many application domains—most notably in credit scoring—the AUC is often transformed into a measure that tends to zero for a completely random classifier and equals one for a perfect classifier (it may also take negative values if the classifier’s predictions are systematically reversed). This transformation ($2\times\text{AUC} - 1$) is commonly referred to as the Gini coefficient in the credit scoring literature. As we will see later, the same measure appears under different names in other fields.

Figure \@ref(fig:auc-gini) illustrates an empirical ROC curve defined by eight discrete cut-off points, along with the areas corresponding to the AUC and the Gini coefficient.

```{r auc-gini, fig.align = "center", out.width = "100%", fig.cap="Geometric interpretation. The black piecewise linear curve represents the receiver operating characteristic (ROC) curve. The AUC (also referred to as AUROC, ROC score, C-statistic, or Vargha–Delaney A) is defined as the area under the ROC curve. A linear transformation of the AUC—known, among others, as the Gini coefficient, Cliff’s delta, Somers’ D, Accuracy Ratio, or the ROC skill score—equals twice the area between the diagonal line y=x and the ROC curve. ", dev=if(knitr::is_latex_output()) "cairo_pdf" else "png"}
cz = c(.2, .42, .6, .7, .8, .88, .95, .99)
sw = c(.98, .95, .9, .86, .8, .72, .6, .4)

df <- data.frame(FPR = c(0, 1-sw, 1), TPR = c(0, cz, 1))

library(ggplot2)
library(patchwork)

ggplot(df, aes(x = FPR, y = TPR)) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0, ymax = 1, fill = "white", color = "black") +
  geom_area(fill="lightblue") +
  geom_line() +
  geom_point() +
  annotate("text", x = 0.5, y = 0.5, label = "AUC", size =10,  color = "darkblue") +
  coord_fixed(ratio = 1, xlim = c(0,1), ylim = c(0,1)) +
  labs(x = "FPR (1 - specificity)", 
       y = "TPR (sensitivity)") +
  theme_minimal() +

#library(ggplot2)
ggplot(df, aes(x = FPR, y = TPR)) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0, ymax = 1, fill = "white", color = "black") +
  geom_polygon(fill="orange2") +
  geom_polygon(aes(x=TPR, y=FPR), fill="orange1") +
  geom_line() +
  geom_point() +
  annotate("text", x = 0.5, y = 0.5, label = "“Gini”", size =10,  color = "orange4") +
  coord_fixed(ratio = 1, xlim = c(0,1), ylim = c(0,1)) +
  labs(x = "FPR (1 - specificity)", y = "") +
  theme_minimal()
```

# AUC – alternative names and related measures

The Area Under the Receiver Operating Characteristic (ROC) curve appears in the literature under numerous aliases and mathematical formulations. Recognizing the equivalence of these measures is essential for correct interpretation, comparison of results across disciplines, and informed use of software implementations. As shown below, the AUC is also referred to as the *AUROC*, *ROC score*, *concordance statistic* (*C-statistic*), and the *Vargha-Delaney A statistic*. The AUC is closely related to the Mann-Whitney *U* statistic and is used as a measure of effect size in the context of the Mann-Whitney test and related procedures, such as the Brunner-Munzel test. In this context, it is often referred to as the *probability of superiority*, *relative treatment effect* or a *measure of stochastic superiority*. 

The Gini coefficient, which is a simple linear transformation of the AUC, is also known as *Cliff's delta*, the *ROC skill score* (ROCSS), or the *Accuracy Ratio*. It is a special case of *Somers' D statistic* and is occasionally viewed as a variant of the *rank-biserial correlation*. 

In medical statistics and epidemiology, particularly when evaluating logistic regression models with binary outcomes, the AUC is frequently referred to as the *C-statistic* or *concordance statistic* [@Austin_Steyerberg_2012]. This term was introduced by @Harrell_Califf_Pryor_Lee_Rosati_1982 as representing the probability that a randomly selected positive instance receives a higher predicted score than a randomly selected negative instance.

This probabilistic interpretation serves as the bridge to non-parametric statistics. The U statistic in the Mann–Whitney test represents the number of correctly ordered pairs in all pairwise comparisons between the two groups, where one observation comes from each group, with ties typically counted as 0.5. It can be shown that the area under the ROC curve is equivalent to the Mann-Whitney U statistic divided by the product of the sample sizes of the two groups:

$$\text{AUC} = \frac{U}{n_1  n_0}$$ 

This equality allows confidence intervals for AUC to be derived from the distributional properties of the Mann–Whitney U statistic.

In the context of stochastic ordering tests, such as Mann-Whitney or Brunner–Munzel, the AUC can serve as a measure of effect size and be interpreted as the *probability of superiority*, as it represents the the likelihood that a randomly chosen positive case ranks higher than a randomly chosen negative case [@Hanley_McNeil_1982; @Bamber_1975]. It is also be referred to as a *relative treatment effect* or *stochastic superiority statistic* in this context [@Brunner_Munzel_2000; @Karch_2021].

In the behavioral sciences and psychology, similar concepts have been developed to quantify effect sizes. The *Common Language Effect Size* (CL/CLES) [@McGraw_Wong_1992] defines an effect size as, again, the probability that a score sampled at random from one distribution exceeds a score sampled from another. The name reflects its purpose: to provide an intuitively interpretable metric for audiences without formal statistical training. While the original formulation by McGraw and Wong assumed normal distributions, the measure was later generalized and, under the name *A measure of stochastic superiority*, or *Vargha–Delaney A* [@Vargha_Delaney_2000], became mathematically equivalent to the AUC.

While the AUC ranges from 0.5 (random classification) to 1.0 (perfect classification), many disciplines prefer a measure scaled symmetrically around zero to express the strength and direction of association or discriminatory power. This motivates the use of the Gini coefficient (or Gini index). In classification settings, the Gini coefficient is a simple linear transformation of the AUC, defined as [@Hand_Till_2001]: 

$$\text{Gini} = 2 \times \text{AUC} - 1$$ 

The term “Gini coefficient” is used predominantly in the credit scoring literature [@Anderson_2007; @Rezac_Rezac_2011]. To distinguish it from the classical Gini index in economics, which measures income or wealth inequality, the name *pseudo-Gini* is sometimes employed [@Idczak_2019]. In other application domains, mathematically equivalent quantities may appear under different names.

By construction, the Gini coefficient takes values in (-1, 1) with zero indicating no discriminatory power and negative values corresponding to systematically reversed predictions. Owing to this symmetry, it admits an interpretation analogous to that of a correlation coefficient. Indeed, some authors, such as @Wendt_1972, have proposed the rank-biserial correlation in a form that is mathematically equivalent to the Gini coefficient, although this term is occasionally used for closely related but non-identical formulations [@Rubia_2022]. 

An alternative derivation, independent of ROC analysis, arises from the Cumulative Accuracy Profile (CAP) curve [@Sobehart_Keenan_Stein_2000]. The CAP plots the cumulative proportion of positive cases against the cumulative proportion of the population, sorted by predicted scores. From this curve, the *Accuracy Ratio* (*AR*) is defined as the ratio of the area between the model CAP and the random CAP to the area between the ideal CAP and the random CAP. It can be shown that

$$\text{AUC} = \frac{\text{AR} + 1}{2}$$
 
[@Engelmann_Hayden_Tasche_2003], demonstrating that although AR originates from the CAP curve rather than the ROC curve, it conveys exactly the same information as the Gini coefficient.

Just like AUC, the Gini coefficient admits a probabilistic interpretation. If we randomly sample one positive and one negative case and compare their scores, the Gini coefficient equals the difference between the probability that the positive case receives a higher score than the negative case and the probability of the reverse. @Agresti_2010 denotes AUC by $\alpha$ and the Gini coefficient by $\Delta$, referring to both as measures of *stochastic superiority*. @Cliff_1993 describes the effect size measure $d$ (or $\delta$ for the population), which adopts a similar probabilistic perspective on rank-based comparisons. This measure—identical to the Gini coefficient—is later sometimes referred to as *Cliff’s delta* [@Bais_Van_Der_Neut_2022].

Somers’ D is a measure of association between a predictor X and an ordinal dependent variable, based on the comparison of concordant and discordant pairs [@Somers_1962]. When Y is binary, Somers’ D reduces to 2AUC-1 [@Newson_2002]. Thus, in binary classification settings, the Gini coefficient can be interpreted as Somers’ D with a binary response variable.

It is worth mentioning that ROC curves (under the name of _relative_ operating characteristics curves) are also used in meteorology to assess probabilistic forecasts for binary events. In this context, the AUC may be referred to as the ROC score, while the Gini coefficient is known as the ROC skill score [@Kharin_Zwiers_2003; @Mason_Graham_1999]. 

# The need for computational efficiency in AUC estimation

In the context of modern statistical modeling and machine learning, a
single calculation of the AUC metric is often insufficient for a
comprehensive assessment of model quality. Complex validation,
optimization, and simulation procedures require the AUC statistic to be
computed repeatedly, imposing high demands on the computational
efficiency of the underlying algorithms. The following sections outline
key areas where the performance of AUC estimation is critical.

**Uncertainty estimation and permutation tests**

The estimation of confidence intervals and the verification of
statistical hypotheses for the AUC often rely on resampling methods,
such as bootstrapping or permutation tests. While necessary when simple
parametric assumptions cannot be met, these methods generate significant
computational loads.

In the case of massive datasets, the process of generating a single
performance estimate can be computationally expensive
[@LeDell_Petersen_Van_Der_Laan_2015]. The authors point out that when
using complex prediction methods, even with relatively small datasets,
cross-validation can consume a large amount of time, making the
bootstrap a computationally intractable approach to variance estimation
in many practical settings.

Similarly, in the context of meta-analysis of diagnostic accuracy
studies, there are instances of bootstrap algorithms usage to determine
confidence intervals for the AUC of the Summary ROC (SROC) curve
[@Noma_Matsushima_Ishii_2021]. Their approach involves computing AUC
estimates from a large number of bootstrap samples (e.g., $B=1000$),
which necessitates efficient calculation routines.

Permutation-based inference also demands repeated calculation. Bandos,
Rockette, and Gur (2006) developed a permutation test for comparing ROC
curves in multireader studies. An exact permutation test in this setting
is formed by determining the frequency of the statistic—estimating the
average difference in AUCs across all possible exchanges of reader
ratings. For larger samples, this requires an asymptotic approach due to
the computational intensity of calculating differences for every
permutation [@Bandos_Rockette_Gur_2006]. Furthermore, Pauly, Asendorf,
and Konietschke (2016) proposed rank-based studentized permutation
methods for the nonparametric Behrens-Fisher problem, which corresponds
to inference for the AUC. They demonstrated that the studentized
permutation distribution ofthe Brunner-Munzel rank statistic is
asymptotically standard normal, providing a theoretical foundation for
consistent confidence intervals and tests, which rely on these intensive
permutation procedures [@Pauly_Asendorf_Konietschke_2016].

**Variable importance measures**

Efficient AUC computation is a prerequisite for specific Variable
Importance Measures (VIM). Therefore an AUC-based permutation variable
importance measure for Random Forests was introduced, designed to be
more robust to unbalanced classes than standard error-rate-based
measures. This procedure requires the AUC to be computed for each tree
in the forest, both before and after permuting the values of a given
predictor. Given the number of trees in a forest and the number of
predictors, this approach results in a vast number of AUC calculations,
far exceeding the computational cost of standard single-pass metrics
[@Janitza_Strobl_Boulesteix_2013].

Beyond model-specific methods, model-agnostic interpretability
frameworks also rely heavily on repeated metric estimation. The DALEX
package [@Biecek_2018] implements a permutation-based variable
importance method applicable to any predictive model. This approach
measures the change in model performance, such as the drop in AUC
(represented as the loss function $L(y, \hat{y}) = 1 - AUC$), after
permuting the values of a single predictor. To ensure stability of the
importance estimates, this permutation process is typically repeated $B$
times (e.g., $B=10$) for every feature in the dataset. Consequently, for
a model with $p$ features, assessing global feature importance requires
$p \times B$ independent AUC calculations, creating a linear dependency
between the number of features and the computational cost.

**AUC maximization algorithms**

In the era of Big Data, learning algorithms that directly maximize AUC,
rather than accuracy, are gaining importance but algorithms maximizing
model accuracy do not necessarily maximize the AUC score. However,
direct AUC maximization presents a computational challenge because the
function is non-decomposable over individual examples. This has led to
the development of stochastic AUC maximization methods for big data and
"Deep AUC Maximization" (DAM) for deep learning, where optimization must
be performed efficiently on large-scale datasets [@Yang_Ying_2023].

**Simulation and curve modeling in credit risk**

In credit risk management, theoretical ROC curve models are employed to
simulate and assess the impact of scoring models when actual data is
unavailable or limited. Kochański (2022) notes that fitting models such
as the binormal or bigamma curves is helpful for "generating ROC curves
without underlying data". This capability is essential for assessing the
impact of a credit scorecard that is "yet to be built"
[@Kochanski_2022]. Such simulation analyses often involve generating
numerous curves and estimating their parameters to forecast portfolio
quality, further justifying the need for computationally efficient AUC
estimation methods.

Beyond single-curve fitting, Kochański (2021) extends the analysis to a
dynamic market environment, proposing a simulation model for risk and
pricing competition. This framework explores how the discrimination
power of credit scoring models influences key business metrics,
illustrating the "trade-off between profitability, market share, and
credit loss rates" [@Kochanski_2021]. The study demonstrates that even
marginal improvements in discrimination power can yield substantial
benefits, a conclusion derived from simulation scenarios that model the
interactions between lenders and borrowers. Such comprehensive market
analyses require processing numerous scenarios to identify
profit-maximizing strategies, further highlighting the role of
performance metrics as fundamental parameters in complex economic
models.

# R packages

```{r}
ftl <- read.csv("../data/functions_table.csv", sep = ";")

fth <- as.data.frame(lapply(ftl, function(x) {
  x %>%
    gsub("\\$", "\\\\$", .) %>%
    gsub("@", "\\\\@", .)
  }))

if (knitr::is_latex_output()) {
  knitr::kable(ftl, format = "latex", booktabs = TRUE, linesep = "\\addlinespace",
               caption = "Functions computing AUC.") %>%
    kable_styling(latex_options = c("HOLD_position", "scale_down")) %>%
    column_spec(3, monospace = TRUE)
} else {
  knitr::kable(fth, format = "html",
             caption = "Functions computing AUC.") %>%
    kable_styling(bootstrap_options = "condensed") %>%
    column_spec(3, width_min = "33em", width_max = "33em", monospace = TRUE) %>%
    column_spec(4, width_min = "9em", width_max = "9em")
}
```

We identified three main types of algorithms for AUC computation: (1)
trapezoidal integration over the ROC Curve, (2) (optimized) pairwise
comparison, (3) and rank-based (Mann–Whitney U statistic formulation).

Let $(s_i, y_i)$ for $i = 1, \dots, n$ denote the score assigned to an
account and its corresponding true label. In line with standard practice
in credit scoring, we assume that $y_i=1$ indicates a "bad" account
(e.g., one that defaults, doesn't repay the loan), while $y_i=0$
represents a good account. A properly functioning scoring should assign
lower scores to accounts with a higher predicted probability of being
bad, and higher scores to those likely to be good.

Let $n_1 = \sum_{i=1}^{n}\mathbb{I}(y_i=1)$ denote the number of bad
accounts, and $n_0 = \sum_{i=1}^{n}\mathbb{I}(y_i=0)$ – number of good
accounts.

**Trapezoidal integration over the ROC curve**

$$\text{AUC} = \sum_{k=1}^{m-1}(\text{FPR}_{k+1} - \text{FPR}_{k})\cdot\frac{\text{TPR}_{k+1}+\text{TPR}_k}{2}$$

where $m$ is the number of distinct score thresholds from lowest to the
highest score, $TPR_k$ is the True Positive Rate, $FPR_k$ is the False
Positive Rate. $TPR_k=TP_k/n_1$ and $FPR_k=FP_k/n_0$, where $TP_k$ is
the number of true positives, $\text{FP}_k$ is the number of false
positives, $n_1$ is the number of positive cases, $n_0$ is the number of
negative cases.

**Optimized pairwise comparison**

AUC as the probability that a randomly chosen positive instance receives
a higher score than a randomly chosen negative instance:

$$\text{AUC} = \frac{1}{n_1 n_0}\sum_{i:y_i=1}\sum_{j:y_j=0}\left[\mathbb{I}(s_i<s_j)+\frac{1}{2}\mathbb{I}(s_i=s_j)\right]$$

**Rank-based (Mann–Whitney U statistic formulation)**

$$\text{AUC} = \frac{\bar{R}_{1}-{n_1(n_1+1)}/2}{n_0}$$

where $\bar{R}_{1}$ is the mean rank for $s_i$ where $y_i=1$:

$$\bar{R}_{1} = \frac{1}{n_1}\sum_{i:y_i=1}\text{Rank}(s_i)$$

# Efficiency study

## Benchmarking packages

All of the R packages that can be used to calculate the AUC measure have
had their performance speed tested. In every benchmark, each function
was called 100 times, and there are a total of three benchmarks, each
with a different number of observations. The predictor *pred* has been
generated from the normal distribution, with the first half of the
observations using $N(0, 1)$, and the second half being $N(1, 1)$. The
*target* variable is a vector, where the first half of values are all
equal to 1 and the second half of values are all equal to 0.

```{r setting up for package benchmarks, include = FALSE}
plot_benchmark <- function(benchmark) {
  autoplot(benchmark) +
    theme(axis.text.y = element_text(size = rel(1.5)))
}

package_benchmark1 <- readRDS("../data/packages_benchmark_1.rds")
package_benchmark2 <- readRDS("../data/packages_benchmark_2.rds")
package_benchmark3 <- readRDS("../data/packages_benchmark_3.rds")

```

**First benchmark - 1000 observations**

```{r package_benchmark_1, fig.align="center", out.width="100%"}
plot_benchmark(package_benchmark1)

```

**Second benchmark - 10000 observations**

```{r package_benchmark_2, fig.align="center", out.width="100%"}
plot_benchmark(package_benchmark2)

```

**Third benchmark - 100000 observations**

```{r package_benchmark_3, fig.align="center", out.width="100%"}
plot_benchmark(package_benchmark3)

```

The time needed to complete the calculations varies greatly and spans
multiple orders of magnitude. For example, using *bigstatsr* to
calculate the AUC metrics is almost 100 times faster than by using the
*scorecard* package. There may be different causes to this, for example
a function might additionally perform other calculations, like
generating confidence intervals or also calculating other metrics, or
otherwise due to the algorithms used being inefficient, or perhaps even
both of these issues at once.

In general, *bigstatsr* offers the fastest way of computation overall.
While *MLMetrics* and *Hmisc* are slightly faster on the smallest data
set, *bigstatsr* noticeably outperforms them as the number of
observations increases.

It is therefore recommended to use the *AUC* function from the
*bigstatsr* package whenever calculating the AUC metric in R, whenever
an already existing function is desired.

## Benchmarking the algorithms

The three identified algorithms have been tested against each other, by
creating as optimised functions implementing them as possible. They only
contain the calculation, with no checks whether the inputs are correct,
to minimise potential interference and isolate the algorithms. They have
been performed on the same observations as the package benchmarks.

```{r setup for own function benchmarks, include = FALSE}
own_functions_benchmark1 <- readRDS("../data/own_functions_benchmark_1.rds")
own_functions_benchmark2 <- readRDS("../data/own_functions_benchmark_2.rds")
own_functions_benchmark3 <- readRDS("../data/own_functions_benchmark_3.rds")

```

**First benchmark - 1000 observations**

```{r own_function_benchmark_1, fig.align="center", out.width="100%"}
plot_benchmark(own_functions_benchmark1)
```

**Second benchmark - 10000 observations**

```{r own_function_benchmark_2, fig.align="center", out.width="100%"}
plot_benchmark(own_functions_benchmark2)
```

**Third benchmark - 100000 observations**

```{r own_function_benchmark_3, fig.align="center", out.width="100%"}
plot_benchmark(own_functions_benchmark3)
```

In all the benchmarks, the optimised pairwise comparison method is the
fastest, with the trapezoid method being only slightly slower. The
rank-sum method becomes noticeably slower as the number of observations
increases. As a result, whenever developing a new function to calculate
the AUC metric, it is recommended to implement it using the pairwise
comparison method.
